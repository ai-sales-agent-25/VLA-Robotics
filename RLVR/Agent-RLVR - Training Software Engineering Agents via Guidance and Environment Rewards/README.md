https://arxiv.org/abs/2506.11425

**Agent-RLVR: Training Software Engineering Agents via Guidance and Environment Rewards**

### 1. Summary and Rating

This paper introduces Agent-RLVR, a framework designed to make Reinforcement Learning from Verifiable Rewards (RLVR) effective for complex, multi-step agentic tasks, with a specific focus on software engineering (SWE). The authors identify that conventional RLVR fails in these settings due to the sparse reward landscape—an agent rarely succeeds in solving a complex coding issue from scratch, providing insufficient positive feedback for learning. To overcome this, Agent-RLVR introduces "agent guidance," a pedagogical mechanism where a "teacher" model provides targeted hints after an agent's initial failed attempt. This guidance, which includes strategic plans, feedback on errors, and pointers to relevant files, steers the agent towards a successful trajectory.

The training loop involves an initial unguided attempt, followed by a guided re-attempt on failure. The resulting pairs of successful (guided) and unsuccessful (unguided) trajectories are used to create preference data for training the agent's policy via Direct Policy Optimization (DPO). Using a curated dataset of 817 SWE environments, the authors demonstrate that Agent-RLVR significantly improves the PASS@1 performance of a Qwen-2.5-72B model on the SWE-Bench Verified benchmark from 9.4% to 22.4%. They further show that this guidance-augmented data can be used to train a test-time reward model, which boosts performance to 27.8%.

**Rating: 9/10**

The paper presents a method that is conceptually simple yet highly effective in addressing a well-known and critical challenge in training LLM agents: learning from sparse rewards in complex environments. The "agent guidance" mechanism is a practical and well-motivated solution that demonstrably bridges the gap between agent capability and task difficulty. The empirical results are strong and convincing, showing a >2x improvement on a challenging benchmark with a relatively small amount of training data. The work is methodologically sound, with clear ablations and a well-structured presentation, making it a valuable and impactful contribution for researchers and practitioners working on agentic AI.

### 2. Main Ideas

1.  **Agent Guidance to Overcome Sparse Rewards in RLVR:** The central idea is that standard RLVR is ineffective for agentic SWE tasks because success is rare, leading to sparse rewards. The paper introduces "agent guidance"—pedagogical hints generated by a teacher model after an initial failure—to steer the agent toward a correct solution. This process effectively densifies the reward signal by creating successful learning examples for problems the agent could not solve on its own.
2.  **A Two-Stage Training Loop for Data Generation:** The Agent-RLVR framework employs a specific two-stage process. First, the agent attempts a task without help. If it fails, guidance is generated, and the agent re-attempts the task. This process generates rich preference data (successful guided trajectory vs. unsuccessful initial trajectory) that is ideal for offline RL algorithms like DPO, allowing the agent to learn from its guided successes.
3.  **Synergistic Use of Generated Data for Reward Modeling:** The paper demonstrates that the preference data collected via the guidance process has value beyond direct policy training. This data can also be used to train an effective reward model. At test time, this reward model can rank multiple candidate solutions generated by the policy model, leading to a substantial additional improvement in performance and highlighting the versatility of the data generated by the Agent-RLVR framework.

### 3. Top 10 Most Important Citations

1.  **Rafailov et al. (2023)** Direct preference optimization: Your language model is secretly a reward model.
    This paper introduces Direct Policy Optimization (DPO), the core reinforcement learning algorithm used in Agent-RLVR to fine-tune the policy model on pairs of winning and losing trajectories.
    (Link: https://arxiv.org/abs/2305.18290)

2.  **Chowdhury et al. (2024)** Introducing SWE-bench verified.
    This paper provides the specific, human-validated version of the SWE-bench benchmark that Agent-RLVR uses for evaluation, ensuring the reliability of its reported performance metrics.

3.  **Xia et al. (2024)** Agentless: Demystifying llm-based software engineering agents.
    This work introduces the "Agentless" scaffolding, a popular framework for SWE agent evaluation, which Agent-RLVR adapts and simplifies for its experimental setup.
    (Link: https://arxiv.org/abs/2407.01489)

4.  **Lambert et al. (2024)** Tūlu 3: Pushing frontiers in open language model post-training.
    This paper is credited with coining the term Reinforcement Learning from Verifiable Rewards (RLVR), defining the central research area that Agent-RLVR aims to advance into more complex agentic domains.
    (Link: https://arxiv.org/abs/2411.15124)

5.  **DeepSeek-AI et al. (2025)** Deepseek-r1: Incentivizing reasoning capability in ilms via reinforcement learning.
    This paper demonstrates the success of RLVR on verifiable tasks like math and coding, serving as a key reference point that highlights the difficulty of applying the same methods to agentic settings, which motivates the need for Agent-RLVR.
    (Link: https://arxiv.org/abs/2501.12948)

6.  **Ouyang et al. (2022)** Training language models to follow instructions with human feedback.
    This is a foundational paper (InstructGPT) that established the paradigm of using preference-based feedback to align LLMs, which underpins the RLVR and DPO methods used in this work.
    (Link: https://arxiv.org/abs/2203.02155)

7.  **Wei et al. (2025)** Swe-rl: Advancing ilm reasoning via reinforcement learning on open software evolution.
    This is a significant, concurrent work that also applies reinforcement learning to SWE tasks, making it a critical point of comparison for evaluating the performance and methodology of Agent-RLVR against the state of the art.
    (Link: https://arxiv.org/abs/2502.18449)

8.  **Jimenez et al. (2023)** Swe-bench: Can language models resolve real-world github issues?
    This paper introduced the original SWE-bench benchmark, which created the entire task category of autonomous software engineering in real-world codebases that Agent-RLVR is designed to solve.
    (Link: https://arxiv.org/abs/2310.06770)

9.  **Pan et al. (2024)** Training software engineering agents and verifiers with swe-gym.
    The SWE-Gym project is a key source of training environments for the dataset curated in this paper, providing a foundation of problems on which to apply the Agent-RLVR training loop.
    (Link: https://arxiv.org/abs/2412.21139)

10. **Zelikman et al. (2022)** STaR: Bootstrapping reasoning with reasoning.
    This work's concept of using a model to generate rationales to solve problems it initially failed on is conceptually similar to Agent-RLVR's use of guidance, serving as an important intellectual precedent for self-improvement through guided re-attempts.
