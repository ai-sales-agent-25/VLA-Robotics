https://arxiv.org/abs/2507.14843

https://x.com/WUFang40615703/status/1947740481032708276

**The Invisible Leash: Why RLVR May Not Escape Its Origin**

### 1. Summary and Rating

This paper presents a theoretical and empirical investigation into the limitations of Reinforcement Learning with Verifiable Rewards (RLVR) for enhancing the reasoning capabilities of large language models. The central thesis is that RLVR, in its current formulation, acts more as a conservative reweighting mechanism than a method for discovering genuinely novel reasoning pathways.

Theoretically, the authors formalize the concept of "support preservation," arguing that on-policy RL methods cannot assign probability to solutions that have zero initial probability under the base model. This effectively creates an "invisible leash," tethering the trained model to the knowledge implicitly contained within its initial parameters. They frame RLVR through the lens of variational inference, showing it finds the closest possible distribution to the base model that satisfies a reward constraint, thus favoring minimal changes. This leads to a fundamental trade-off between precision and diversity, where optimizing for rewards systematically reduces the entropy of the output distribution.

Empirically, the paper validates these theoretical claims across a wide range of math and general reasoning benchmarks. The authors introduce the concepts of "empirical-support shrinkage" (losing correct answers the base model could find) and "expansion" (discovering new correct answers). Their key finding is that while RLVR consistently improves precision (pass@1), shrinkage generally outweighs expansion, meaning the model loses access to more correct solutions than it gains. Furthermore, they analyze entropy dynamics, showing that even when token-level entropy increases (suggesting more local uncertainty in the generation process), the final answer-level entropy consistently decreases, indicating a collapse onto a narrower set of solutions. The paper concludes that breaking this "invisible leash" will require new algorithmic approaches, such as explicit exploration strategies or off-policy methods, to seed probability into underrepresented solution regions.

**Rating: 9/10**

This is a high-quality paper that addresses a timely and critical question in the development of advanced reasoning models. The "invisible leash" is a powerful and intuitive metaphor, and the authors support it with a clear theoretical framework (support preservation, variational interpretation) and rigorous, large-scale empirical evidence. The distinction between support shrinkage and expansion provides a valuable new lens for analyzing the effects of RL fine-tuning. For a PhD-level audience, the paper's strength lies in its unified perspective that connects theoretical principles to concrete, observable phenomena, and its nuanced analysis of entropy dynamics is particularly insightful. It clearly defines the limitations of a popular method and provides a well-grounded direction for future research.

### 2. Main Ideas

1.  **Support Preservation as an "Invisible Leash":** The core theoretical argument is that RLVR is fundamentally constrained by the support of the base model's distribution. Because on-policy RL relies on sampling from the current policy to generate experience for updates, it cannot discover solutions that the base model assigns zero (or practically zero) probability to. This means RLVR is inherently a conservative optimization process that reweights probabilities within its existing reach, rather than a mechanism for true exploration or discovery of novel reasoning patterns.

2.  **Empirical-Support Shrinkage Outweighs Expansion:** The paper's primary empirical finding is that RLVR's tendency to narrow the distribution of answers has a significant side effect. While it can occasionally amplify the probability of a previously "hidden" correct answer ("expansion"), it more frequently prunes away low-probability but correct reasoning paths that the base model could access ("shrinkage"). Across numerous benchmarks, the authors demonstrate that the number of correct solutions lost to shrinkage is greater than the number gained through expansion, highlighting a trade-off where gains in precision come at the cost of reasoning diversity and robustness.

3.  **Decoupling of Token-Level and Answer-Level Entropy:** The paper identifies a nuanced dynamic where local uncertainty does not translate to global exploration. The authors show that while RLVR can sometimes increase token-level entropy (making individual generation steps appear more stochastic), the answer-level entropy consistently decreases. This indicates that even if the reasoning paths are more varied or complex, they ultimately converge to a smaller, less diverse set of final answers. This finding challenges the simple interpretation of token-level entropy as a proxy for exploration and underscores RLVR's role in sharpening distributions toward a few high-reward modes.

### 3. 10 Most Important Citations

1.  **Schulman et al. 2017. Proximal policy optimization algorithms.** This paper introduces PPO, a foundational reinforcement learning algorithm that is widely used for training large language models with RLVR and is relevant to the policy update mechanisms discussed.
2.  **Guo et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.** This is cited as a prominent example of a state-of-the-art reasoning model developed using RLVR, establishing the context and relevance of the paper's investigation. [https://arxiv.org/abs/2501.07570](https://arxiv.org/abs/2501.07570)
3.  **Liu et al. 2025. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models.** The authors adopt ProRL as their primary experimental method, making this work essential to the paper's empirical setup and claims.
4.  **Yue et al. 2025a. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model?** This is a key paper framing the central debate, as it highlights the paradoxical failure mode where RLVR models underperform base models at high sampling budgets, a phenomenon this paper seeks to explain. [https://arxiv.org/abs/2504.13837](https://arxiv.org/abs/2504.13837)
5.  **Shao et al. 2025. Spurious rewards: Rethinking training signals in rlvr.** This citation supports the critical perspective on RLVR by questioning whether observed improvements reflect genuine reasoning enhancements, aligning with this paper's investigation into RLVR's true effects.
6.  **Hendrycks et al. 2021. Measuring mathematical problem solving with the math dataset.** This work introduced the MATH dataset, a standard and challenging benchmark for mathematical reasoning that is used extensively in the paper's experiments.
7.  **Lewkowycz et al. 2022. Solving quantitative reasoning problems with language models.** This paper introduced the Minerva models and benchmarks, which are used in the paper's experiments to evaluate mathematical reasoning.
8.  **Stojanovski et al. 2025. Reasoning gym: Reasoning environments for reinforcement learning with verifiable rewards.** Reasoning Gym is a key benchmark used for evaluating a wide range of non-math reasoning tasks, forming a core part of the paper's empirical validation. [https://arxiv.org/abs/2505.24760](https://arxiv.org/abs/2505.24760)
9.  **Kool et al. 2019. Buy 4 reinforce samples, get a baseline for free!** This paper is referenced as an example of an RLVR algorithm (RLOO), placing the current work within the broader context of different RL-based optimization techniques for sequence generation.
10. **Zhao et al. 2024. Sharp analysis for kl-regularized contextual bandits and rlhf.** This work is cited for its analysis of similar exponential tilting policy updates in RLHF, providing a theoretical parallel to the paper's variational interpretation of the RLVR objective.
