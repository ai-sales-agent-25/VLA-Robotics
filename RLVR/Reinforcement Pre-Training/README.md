https://huggingface.co/papers/2506.08007

Reinforcement Pre-Training

### 1. Summary and Rating

This paper introduces Reinforcement Pre-Training (RPT), a novel paradigm that integrates reinforcement learning (RL) directly into the pre-training phase of large language models. Instead of a standard next-token prediction objective, RPT frames the task as a reasoning problem. For a given context from a pre-training corpus, the model first generates an intermediate "chain-of-thought" reasoning trace before predicting the next token. It then receives a verifiable, binary reward (1 for correct, 0 for incorrect) based on whether its final prediction matches the ground-truth token in the corpus. This approach allows RL to be scaled to massive, unannotated text corpora, transforming them into vast datasets for general-purpose RL. The authors demonstrate that RPT improves next-token prediction accuracy, enhances zero-shot performance on challenging reasoning benchmarks (outperforming even larger, conventionally trained models), and provides a stronger foundation for subsequent RL fine-tuning.

**Rating: 9/10**

I rate this paper a 9/10. The core idea of reframing next-token prediction as a scalable, self-supervised RL problem is highly novel and conceptually elegant. It cleverly integrates principles of test-time reasoning (e.g., chain-of-thought) directly into the pre-training objective, effectively spending more computation on challenging tokens during training. The experimental validation is strong, demonstrating clear improvements in next-token prediction, downstream task performance, and favorable scaling properties on a challenging mathematical domain. While the generalizability to web-scale, non-mathematical corpora remains an open question (as the authors note), the work presents a compelling and potentially foundational new direction for pre-training, making it a significant contribution to the field.

### 2. Main Ideas

1.  **Reframing Next-Token Prediction as a Scalable RL Task:** The central idea is to move beyond simple supervised learning for pre-training. RPT treats each next-token prediction as a reasoning task where the model must generate an internal thought process before committing to an answer. The reward signal is verifiable, intrinsic to the data (the ground-truth next token), and does not require costly human annotation. This fundamentally changes the pre-training objective from pattern matching to incentivized reasoning.

2.  **Scaling General-Purpose RL to Pre-Training Corpora:** Current RL applications in LLMs, like RLHF or RLVR, are typically used for fine-tuning on smaller, domain-specific, or preference-annotated datasets. RPT's key innovation is its scalability. By using the correctness of the next token as a reward, it leverages the same massive, unannotated text corpora used for standard pre-training, effectively creating a general-purpose RL training paradigm at an unprecedented scale.

3.  **Improved Performance and Foundational Capabilities:** The paper demonstrates that this new pre-training method leads to tangible benefits. RPT models show significantly higher next-token prediction accuracy, especially on "hard" tokens requiring more reasoning. This improved foundational capability translates to superior zero-shot performance on complex reasoning benchmarks and makes the model a better starting point for further RL fine-tuning, as it has already been "aligned" with an RL objective.

### 3. 10 Most Important Citations

1.  **Ouyang et al. 2022.** Training language models to follow instructions with human feedback. This paper introduced Reinforcement Learning from Human Feedback (RLHF), the dominant paradigm for post-training alignment, which RPT presents as a fine-tuning method that can be improved upon by a better pre-trained base.
2.  **LMP et al. 2025.** Tulu 3: Pushing frontiers in open language model post-training. This is cited as a key example of Reinforcement Learning with Verifiable Rewards (RLVR), a concept RPT builds on by scaling verifiable rewards from specific Q&A tasks to the entire pre-training corpus.
3.  **GYZ et al. 2025.** Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. This work is critical as the authors use the Deepseek-R1 model as their base model and employ the GRPO algorithm from this paper for their RPT implementation. [https://arxiv.org/abs/2501.12948](https://arxiv.org/abs/2501.12948)
4.  **GSY et al. 2024.** Omni-MATH: A universal Olympiad level mathematic benchmark for large language models. This paper provides the OmniMATH dataset, which is the corpus used for reinforcement pre-training in all experiments, making it fundamental to the empirical results. [https://arxiv.org/abs/2410.07985](https://arxiv.org/abs/2410.07985)
5.  **KMH et al. 2020.** Scaling laws for neural language models. This is a foundational paper on LLM scaling, and the RPT paper explicitly analyzes its own scaling properties with respect to training compute, positioning its findings within this established context. [https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361)
6.  **HBM et al. 2022.** Training compute-optimal large language models. This work (the "Chinchilla" paper) is another canonical scaling law paper cited to frame the importance of training compute, which RPT aims to leverage more effectively by focusing on reasoning. [https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556)
7.  **ZHS et al. 2024.** Quiet-star: Language models can teach themselves to think before speaking. This is cited as the "most relevant work" because it also encourages rationale generation for next-token prediction, but RPT contrasts its correctness-based reward against their helpfulness-based reward to minimize reward hacking. [https://arxiv.org/abs/2403.09629](https://arxiv.org/abs/2403.09629)
8.  **JKL et al. 2024.** Openai ol system card. This is cited to contextualize the concept of "test-time scaling," where more inference compute improves reasoning, and RPT is motivated as a method to integrate this principle into the training-time objective itself. [https://arxiv.org/abs/2412.16720](https://arxiv.org/abs/2412.16720)
9.  **AAA et al. 2023.** GPT-4 technical report. This is cited as the reference for the standard Next-Token Prediction (NTP) objective, which serves as the primary baseline and the paradigm that RPT aims to fundamentally improve upon. [https://arxiv.org/abs/2303.08774](https://arxiv.org/abs/2303.08774)
10. **SZY et al. 2024.** HybridFlow: A flexible and efficient RLHF framework. The authors state they implemented their training framework using the `verl` library from this paper, making it a direct and crucial dependency for their methodology. [https://arxiv.org/abs/2409.19256](https://arxiv.org/abs/2409.19256)
