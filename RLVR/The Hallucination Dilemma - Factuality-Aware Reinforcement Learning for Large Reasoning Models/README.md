https://arxiv.org/abs/2505.24630

The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for Large Reasoning Models

**1. Summary and Rating**

This paper addresses a significant challenge in the development of advanced large language models (LLMs): the tendency for reasoning-oriented reinforcement learning (RL) fine-tuning to increase the prevalence of hallucinations (factually incorrect statements). The authors provide a theoretical analysis identifying three primary factors contributing to this phenomenon: high-variance policy gradients when correct answers are sparse, entropy-induced randomness necessary for exploration, and the model's susceptibility to converging on spurious local optima where it confidently generates incorrect reasoning.

To mitigate this "hallucination dilemma," the paper introduces Factuality-aware Step-wise Policy Optimization (FSPO). FSPO is an innovative RL fine-tuning algorithm that integrates explicit factuality verification at each step of the model's reasoning process. It employs an automated verifier to check if each generated reasoning sentence is entailed by given evidence, yielding step-wise factuality scores. These scores are then used to dynamically adjust token-level advantage values within the RL optimization, thereby incentivizing factual correctness throughout the entire reasoning chain, not just for the final answer.

Experiments conducted with Qwen2.5 and Llama models on mathematical reasoning and standard hallucination benchmarks (TruthfulQA, HaluEval, HalluQA) demonstrate that FSPO effectively reduces hallucinations while simultaneously enhancing reasoning accuracy. The method improves reliability and performance by guiding the policy towards solutions that are not only correct but also generated via faithful and verifiable reasoning.

**Rating: 9/10**

For a PhD-level audience, this paper is highly commendable.
*   **Problem Significance (High):** It tackles a critical and timely issue in LLM research – the trade-off between advanced reasoning capabilities and factual accuracy, particularly in the context of RL fine-tuning.
*   **Theoretical Contribution (Strong):** The theoretical analysis (Theorems 4.1, 4.2, 4.3) of why outcome-based RL exacerbates hallucinations provides valuable insight into the learning dynamics, moving beyond empirical observation to offer an explanatory framework. This includes identifying issues like high-variance gradients, the exploration-exploitation dilemma's impact on entropy, and the problem of spurious local optima.
*   **Novelty of Approach (High):** The proposed FSPO algorithm is a novel contribution. Incorporating step-wise factuality verification directly into the RL reward mechanism by adjusting token-level advantages is a well-motivated and sophisticated approach to encourage more faithful reasoning.
*   **Methodological Soundness (Strong):** The design of FSPO, leveraging automated verification and adapting the GRPO framework, is sound. The experimental setup is comprehensive, utilizing relevant benchmarks for both reasoning and hallucination, multiple base models, and appropriate baselines, including ablation studies.
*   **Clarity and Presentation (Good):** The paper is well-structured and clearly articulates the problem, theoretical underpinnings, proposed method, and experimental results. Figures (like Figure 3 illustrating FSPO) aid understanding.
*   **Impact Potential (High):** If widely adopted or if it inspires similar factuality-aware RL techniques, FSPO could significantly improve the trustworthiness and reliability of reasoning-focused LLMs in critical applications.

The paper makes a strong case for its approach, and the results are promising. A point could potentially be debated regarding the scalability of automated verification for all domains or the generalizability of the specific verifier used, but this is a common challenge in the field and does not significantly detract from the paper's core contributions for its chosen experimental domain.

**2. Main Ideas Discussed**

1.  **RL Fine-Tuning for Reasoning Exacerbates Hallucinations:** A central thesis is that while reinforcement learning (RL) improves LLMs' reasoning abilities (e.g., in mathematical or multi-step Q&A), it paradoxically leads to a higher rate of hallucinations. The paper empirically demonstrates this and provides a theoretical basis for why this occurs.
2.  **Theoretical Causes of Increased Hallucinations in RL-tuned LLMs:** The paper offers a theoretical analysis identifying three key factors within RL training dynamics that contribute to hallucinations:
    *   **High-Variance Gradient:** When optimizing for sparse binary rewards (correct/incorrect final answer), the policy gradient has high variance, especially early in training when correct answers are rare, leading to unstable updates.
    *   **Entropy-Induced Randomness:** To discover rewarding outputs, RL policies must maintain a certain level of entropy for exploration. This inherent randomness increases the likelihood of generating unsupported or false statements.
    *   **Spurious Local Optima:** Standard RL objectives can lead to policies converging on confidently incorrect answers that yield zero reward, with no gradient signal to escape these "flat" regions of the reward landscape.
3.  **Factuality-aware Step-wise Policy Optimization (FSPO) as a Solution:** The primary contribution is FSPO, an RL algorithm designed to mitigate hallucinations by integrating factuality verification at each reasoning step. FSPO adjusts token-level advantage values based on the factual correctness of intermediate steps, providing denser and more factually-grounded reward signals. This encourages the model to learn not just correct final answers but also factually sound reasoning processes.

**3. 10 Most Important Citations**

1.  **Guo et al. 2025. Deepseek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.** This paper describes a prominent RL-tuned reasoning model, establishing the context of models that benefit from improved reasoning via RL but are susceptible to the hallucination dilemma FSPO addresses. (arXiv:2501.12948)
2.  **Shao et al. 2024. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.** This work introduces Group Relative Policy Optimization (GRPO), the RL method upon which FSPO directly builds and extends by incorporating factuality awareness. (arXiv:2402.03300)
3.  **Schulman et al. 2017. Proximal Policy Optimization Algorithms.** This paper introduced PPO, a foundational policy gradient algorithm in RL that influences many subsequent methods, including GRPO and thus FSPO, particularly in how policy updates are stabilized. (arXiv:1707.06347)
4.  **Ji et al. 2023. Survey of Hallucination in Natural Language Generation.** This survey provides a comprehensive overview of the hallucination problem in LLMs, defining the central challenge that the reported paper aims to solve. (ACM Computing Surveys, 55(12):1–38)
5.  **Lin et al. 2022. TruthfulQA: Measuring How Models Mimic Human Falsehoods.** This is a key benchmark dataset used in the paper's experiments to evaluate the truthfulness and reduce hallucinations in LLMs. (ACL 2022, arXiv:2205.01020 - Note: paper cites actual proceedings, link from there if possible, but arXiv provided in context)
6.  **Li et al. 2023. HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models.** Co-authored by one of the current paper's authors, this benchmark is crucial for evaluating FSPO's effectiveness in mitigating hallucinations. (EMNLP 2023, arXiv:2305.10511 - Note: paper cites actual proceedings, link from there if possible, but arXiv provided in context)
7.  **Yang et al. 2018. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.** This dataset is used for training and evaluating reasoning, particularly knowledge-intensive tasks where step-wise factual accuracy, targeted by FSPO, is critical. (EMNLP 2018, arXiv:1809.09600 - Note: paper cites actual proceedings, link from there if possible, but arXiv provided in context)
8.  **Jaech et al. 2024. OpenAI o1 System Card.** This citation (referring to OpenAI's o1 model) represents the state-of-the-art in LLM reasoning, often achieved through RL, highlighting the capabilities that current research strives for and the importance of addressing side-effects like hallucinations. (arXiv:2412.16720)
9.  **Ye et al. 2025. LIMO: Less is More for Reasoning.** This paper is cited in the context of using Chain-of-Thought (CoT) solutions for fine-tuning, a paradigm that improves reasoning but, as the main paper suggests, can still suffer from factuality issues that FSPO aims to correct. (arXiv:2502.03387)
10. **Hendrycks et al. 2021. Measuring Mathematical Problem Solving With the MATH Dataset.** This dataset is a standard benchmark for evaluating mathematical reasoning capabilities of LLMs, a key domain where FSPO's ability to maintain factuality during complex derivations is tested. (NeurIPS 2021, arXiv:2103.03874)
