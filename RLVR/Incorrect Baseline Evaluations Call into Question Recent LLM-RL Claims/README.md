https://safe-lip-9a8.notion.site/Incorrect-Baseline-Evaluations-Call-into-Question-Recent-LLM-RL-Claims-2012f1fbf0ee8094ab8ded1953c15a37

https://x.com/lateinteraction/status/1928148705145934252

Paper Name: Incorrect Baseline Evaluations Call into Question Recent LLM-RL Claims

1.  **Summary and Rating:**
    This blog post by Chandak, Goel, and Prabhu critically examines a trend in recent popular LLM-RL (Large Language Model - Reinforcement Learning) papers that claim significant improvements in reasoning abilities. The authors analyze seven such papers and find a common issue: the baseline performance of the pre-RL models is often substantially underreported compared to official model releases (e.g., Qwen) or standardized evaluations (e.g., from the Sober Reasoning paper). Consequently, the "gains" attributed to RL methods are frequently inflated. In several instances, correctly evaluated pre-RL baselines already outperform or closely match the post-RL model performance. The authors hypothesize that many of these RL improvements might stem from the RL process inadvertently correcting suboptimal evaluation setups (e.g., improper prompting, generation hyperparameters like temperature, or output parsing) or teaching the model to better follow specific evaluation formats, rather than genuinely enhancing the underlying reasoning capabilities. They conclude by advocating for greater transparency, recommending that future releases include open-weight checkpoints and sample-level outputs to allow for better scrutiny and reproducibility.

    **Rating: 8.5/10**
    For a PhD-level audience, this blog post serves as an important and timely methodological critique. Its strength lies in its direct comparison of reported numbers versus established baselines across multiple contemporary papers, highlighting a potentially systemic issue in a rapidly evolving subfield. The analysis, while not presenting novel algorithmic contributions, provides a crucial "reality check" that encourages more rigorous evaluation practices. The arguments are well-supported by pointing to specific discrepancies and common pitfalls (e.g., temperature settings, format adherence). It doesn't lose points for being a blog post as instructed; its directness and focused critique are valuable. The slight deduction is because, by its nature as a blog post analyzing other works without access to their full experimental setups, some of its explanations for discrepancies are necessarily "guesses," albeit educated ones. It raises critical questions and proposes actionable solutions for the community.

2.  **Main Ideas Discussed:**
    1.  **Underreported Baselines Inflate RL Gains:** A primary argument is that many recent LLM-RL papers report baseline (pre-RL) model performances that are significantly lower than those found in official model releases or through standardized, careful evaluations. This discrepancy makes the improvements achieved through RL appear much larger than they might actually be when compared against a robustly evaluated baseline.
    2.  **RL Improvements May Address Evaluation Flaws, Not Enhance Reasoning:** The authors suggest that the observed performance gains from RL might often be attributable to the RL process teaching the model to overcome issues in the evaluation setup itself—such as adhering to specific output formats, using better prompts, or optimizing generation hyperparameters—rather than leading to fundamental improvements in the model's intrinsic reasoning abilities.
    3.  **Call for Increased Transparency and Rigor:** The post strongly advocates for better research practices, including the release of open-weight model checkpoints and sample-level outputs for reported evaluations. This would allow the community to verify claims, understand the true nature of improvements, and ensure more robust progress in the field.

3.  **10 Most Important Citations:**
    (Note: For papers critiqued by the blog, first authors are not always provided in the text. "Authors of..." is used as a placeholder. Years are often implied as "recent".)

    1.  **Authors of "Spurious Rewards: Rethinking Training Signals in RLVR"** et al. Recent. Spurious Rewards: Rethinking Training Signals in RLVR. This paper is critiqued for claiming RL improvements on Qwen2.5-7B where the baseline was underreported, and the actual gains after correcting the baseline are minimal. Link: [Arxiv] (as linked in the blog post text for section 1)
    2.  **Authors of "Maximizing Confidence Alone Improves Reasoning"** et al. Recent. Maximizing Confidence Alone Improves Reasoning. This work is analyzed because its post-RL models often underperform correctly evaluated pre-RL Qwen models, questioning if RL genuinely teaches reasoning or merely mitigates evaluation setup flaws. Link: [Website] (as linked in the blog post text for section 2)
    3.  **Authors of "RL with 1 Example"** et al. Recent. RL with 1 Example. The blog post suggests that while this paper's RL accuracies outperform 0-shot baselines, the underreported base model accuracy makes the improvements seem much larger than they are. Link: [Arxiv] (as linked in the blog post text for section 3)
    4.  **Authors of "Learning to Reason without External Rewards" (INTUITOR)** et al. Recent. Learning to Reason without External Rewards. This paper is examined, with the conclusion that its RL model does not appear to outperform the few-shot accuracy of the original model, suggesting improvements might be due to fixing format-following. Link: [Arxiv] (as linked in the blog post text for section 4)
    5.  **Authors of "Verifree: Reinforcing General Reasoners without Verifiers"** et al. Recent. Verifree: Reinforcing General Reasoners without Verifiers. This paper's RL method does outperform official base model numbers, but the blog critiques its underestimation of various baseline model accuracies, which could mislead about true efficacy. Link: [Arxiv] (as linked in the blog post text for section 5)
    6.  **Authors of "Unreasonable Effectiveness of Entropy Minimization"** et al. Recent. Unreasonable Effectiveness of Entropy Minimization. The blog post notes that for most datasets, this paper underreports base model accuracy, making gains appear larger, though it correctly frames its contribution as eliciting existing capabilities. Link: [Arxiv] (as linked in the blog post text for section 6)
    7.  **Authors of "Can Large Reasoning Models Self-Train?"** et al. Recent. Can Large Reasoning Models Self-Train?. This paper is critiqued because actual base model accuracies are much higher than reported, significantly reducing the perceived gains from their self-training RL algorithm. Link: [Arxiv] (as linked in the blog post text for section 7)
    8.  **Authors of "Sober Reasoning report"** (specific authors not named in blog). Year Unspecified. Sober Reasoning report. This report is repeatedly referenced as a source for standardized evaluation numbers and methodological recommendations (e.g., appropriate temperature settings), forming a key benchmark against which the blog evaluates other papers' claims. Link: No direct link to the report, but "Prompts in the sober reasoning report" is linked on page 3.
    9.  **Authors of Qwen official technical report** (e.g., for Qwen2.5). Year Unspecified (contemporary with model releases). Qwen official technical report(s). These reports are cited as sources for official baseline performance numbers and recommended evaluation settings (e.g., prompts for MATH benchmarks) for Qwen models, which are used to highlight discrepancies in reported baselines. Link: Page 2 links "technical report" for Qwen2.5 MATH prompt example.
    10. **Holtzmann et al.** 2019. The Curious Case of Neural Text Degeneration. This paper is cited to support the argument that evaluating LLMs at temperature 0 can lead to degenerate outputs, especially for complex reasoning tasks, and that higher temperatures are often preferable. Link: Not provided in the blog post.
