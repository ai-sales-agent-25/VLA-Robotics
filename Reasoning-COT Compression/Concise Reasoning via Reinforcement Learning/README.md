https://arxiv.org/abs/2504.05185

**Concise Reasoning via Reinforcement Learning**

**1. Summary and Rating**

This paper investigates the phenomenon of increased response length in large language models (LLMs) after reinforcement learning (RL) for reasoning tasks. The authors argue, through mathematical analysis and empirical results, that this prolixity is not an inherent requirement for improved reasoning accuracy but rather an artifact of RL optimization, particularly Proximal Policy Optimization (PPO) and Grouped Reinforcement Policy Optimization (GRPO). They demonstrate that incorrect answers in PPO (with λ < 1) tend to drive longer responses to minimize loss, while correct answers favor brevity. GRPO, while initially promoting conciseness for positive advantages, is shown to suffer from collapse modes when problem sets are either entirely unsolvable or fully solvable, limiting its reliability.

The core contribution is a two-phase RL training strategy: Phase 1 focuses on enhancing the base model's reasoning capabilities (potentially increasing length), and Phase 2 uses a very small set of occasionally solvable problems to enforce conciseness while maintaining or even improving accuracy. Experiments show this approach can significantly reduce response length (e.g., over 54% for R1 1.5B and 40% for R1 7B models) and improve robustness, particularly at low temperatures, without sacrificing reasoning performance. The paper also highlights a natural correlation where more concise reasoning often aligns with higher accuracy, challenging the prevailing assumption that longer chains of thought are always better.

**Rating: 9/10**

This paper provides a strong contribution by offering a nuanced mathematical understanding of how RL algorithms like PPO and GRPO influence response length in LLMs. The proposed two-phase RL strategy is practical and addresses a significant issue of computational cost and latency in reasoning models. The findings, particularly the effectiveness of training for conciseness with minimal data and the analysis of GRPO's collapse modes, are insightful and valuable for researchers and practitioners working on LLM reasoning. The empirical validation across multiple benchmarks and models is thorough. The slightly dense mathematical sections are justified by the depth of analysis they enable.

**2. Main Ideas Discussed**

1.  **RL-Induced Prolixity as an Optimization Artifact, Not an Accuracy Necessity:** The paper compellingly argues that the tendency of RL-trained LLMs to generate longer responses is primarily a side effect of the loss minimization process in algorithms like PPO (where negative rewards encourage longer exploration to reduce loss) and GRPO (which also has length-influencing dynamics). It challenges the common belief that increased length directly and necessarily causes improved reasoning, showing instead that concise responses often correlate with correctness.
2.  **Two-Phase RL for Reasoning Enhancement and Conciseness Enforcement:** A novel, practical strategy is proposed where an initial RL phase enhances the model's core reasoning abilities (even if it leads to longer responses). This is followed by a targeted second RL phase, using a surprisingly small dataset of occasionally solvable problems, to significantly reduce response length while preserving or even improving accuracy and model robustness.
3.  **Differential Impact of PPO and GRPO on Response Length and Stability:** The paper provides a detailed mathematical analysis of PPO (with GAE) and GRPO. It shows PPO (with λ < 1) favors shorter responses for positive rewards and longer for negative. Crucially, it identifies "collapse modes" in GRPO where the advantage signal diminishes for entirely solved or unsolvable problem batches, causing the KL divergence term to dominate and leading to either stalled progress or overly short, unhelpful responses, making PPO a more robust choice for achieving conciseness.

**3. List of 10 Most Important Citations**

1.  **Sutton et al. 2018.** Reinforcement Learning: An Introduction. This is the foundational textbook for reinforcement learning, providing the core principles and algorithms (like value functions and policy optimization) that the paper builds upon. (URL: http://incompleteideas.net/book/the-book-2nd.html)
2.  **Schulman et al. 2017.** Proximal policy optimization algorithms. This paper introduces the PPO algorithm, which is a central RL method analyzed and utilized for training reasoning models in the current work. (URL: http://arxiv.org/abs/1707.06347)
3.  **Guo et al. 2025.** Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. This paper introduces DeepSeek-R1, a model whose variants are used as baselines and for post-training experiments, representing the state-of-the-art in RL-based reasoning models that the current work aims to make more concise. (arXiv:2501.12948)
4.  **Schulman et al. 2016.** High-dimensional continuous control using generalized advantage estimation. This introduces Generalized Advantage Estimation (GAE), which is used in conjunction with PPO in the paper's analysis (Theorem 1) to understand PPO's impact on response length.
5.  **Shao et al. 2024.** Deepseekmath: Pushing the limits of mathematical reasoning in open language models. This paper is relevant as it likely introduces or heavily features GRPO (which the current paper analyzes as an alternative to PPO) in the context of mathematical reasoning, a domain used in experiments. (Note: The OCR text for ref [15] refers to GRPO, and this paper is by the DeepSeek team, making it the likely GRPO source or a key application paper).
6.  **Liu et al. 2025.** Understanding rl-zero-like training: A critical perspective. This citation is important because the current paper references their argument about GRPO's denominator biasing towards longer responses, offering a more nuanced counter-argument and analysis. (URL: https://arxiv.org/abs/2503.20783)
7.  **Hendrycks et al. 2021.** Measuring mathematical problem solving with the math dataset. This paper introduces the MATH dataset, which is a key benchmark used extensively in the experiments to evaluate reasoning performance and response length. (arXiv:2103.03874)
8.  **Kimi Team et al. 2025.** Kimi k1. 5: Scaling reinforcement learning with llms. This work is cited as an example of recent efforts to scale RL for LLMs and address issues like conciseness, providing context for the current paper's contributions. (arXiv:2501.12599)
9.  **Yeo et al. 2025.** Demystifying long chain-of-thought reasoning in llms. This paper is cited for its work on chain-of-thought length and proposing methods like cosine length-scaling rewards, representing alternative approaches to managing verbosity that the current paper contrasts with its two-phase RL method. (URL: https://arxiv.org/abs/2502.03373)
10. **He et al. 2024.** Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. This benchmark was used for some of the initial PPO experiments (Figure 1) to study the relationship between loss and response length.
