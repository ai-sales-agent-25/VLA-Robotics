https://huggingface.co/papers/2505.19788

**Done Is Better than Perfect: Unlocking Efficient Reasoning by Structured Multi-Turn Decomposition**

1.  **Brief Summary and Rating:**
    This paper introduces Multi-Turn Decomposition (MinD), a novel method to enhance the efficiency of Large Reasoning Models (LRMs) that typically suffer from long, monolithic Chain-of-Thought (CoT) generations, leading to high latency and token usage. MinD reformulates the standard "think-then-answer" CoT into a sequence of explicit, structured, multi-turn interactions. Each turn consists of a distinct "thinking unit" and a corresponding intermediate answer, allowing subsequent turns to reflect, verify, or revise previous steps. The implementation involves a two-stage process: first, supervised fine-tuning (SFT) is used to adapt LRMs to this multi-turn format, with training data generated by prompting another LLM (like GPT-4o) to segment existing CoTs. Second, Group Relative Policy Optimization (GRPO) is applied to encourage the model to produce correct answers using fewer reasoning turns, thereby reducing redundancy and improving efficiency. Experiments on models like DeepSeek-R1-Distill-Qwen show that MinD can reduce output token usage and time-to-first-token (TTFT) by up to ~70% while maintaining competitive accuracy on benchmarks such as MATH-500, AIME24, AMC23, and GPQA-Diamond. The approach also offers users more explicit control over the iterative reasoning process.

    **Rating: 9/10**
    This paper addresses a critical and timely problem in LLM reasoning: efficiency. The proposed MinD framework is an intelligent combination of structured output formatting (multi-turn decomposition) and targeted reinforcement learning (GRPO for turn minimization). The methodology is sound, involving a practical approach to data generation for SFT and a well-justified use of GRPO to optimize for conciseness without sacrificing (and in some OOD cases, improving) accuracy. The empirical results are compelling, demonstrating significant improvements in token efficiency and latency on challenging reasoning benchmarks, including good generalization to out-of-domain tasks. The paper is well-written, clearly articulating the problem, solution, and experimental validation. The concept of explicitly managing "thinking units" and the "done is better than perfect" philosophy for faster, yet still robust, answers is a valuable contribution. Minor points might involve the dependency on a powerful teacher model for data restructuring and the current scope being primarily on mathematical reasoning, but these do not significantly detract from the core contribution and its potential impact.

2.  **Main Ideas Discussed:**

    1.  **Multi-Turn Decomposition (MinD) of CoT:** The core idea is to transform the conventional, lengthy, single-block Chain-of-Thought into a sequence of explicit, structured turns. Each turn encapsulates a single "thinking unit" (e.g., initial attempt, validation, alternative approach) and produces an intermediate answer, allowing for more granular control and iterative refinement of the reasoning process.
    2.  **Two-Stage Training (SFT + GRPO for Efficiency):** MinD is realized through a pipeline:
        *   **Supervised Fine-Tuning (SFT):** First, an LRM is fine-tuned on data where existing CoTs are rephrased (using another LLM like GPT-4o) into the desired multi-turn format. This teaches the model the new interaction paradigm.
        *   **Reinforcement Learning (GRPO):** Subsequently, Group Relative Policy Optimization is used to further refine the model, specifically to encourage it to generate correct answers in fewer reasoning turns. This addresses the issue that SFT alone might not reduce redundancy and could even increase token count due to added intermediate answer tokens. GRPO (with specific reward components like `R_unit`) penalizes overly cautious, redundant multi-turn outputs.
    3.  **Addressing "Thinking Unit" Redundancy for Efficiency Gains:** The paper empirically shows that standard LRMs often employ redundant thinking units within their CoTs (e.g., multiple verifications yielding the same conclusion). MinD aims to make these units explicit and then reduce their number through the GRPO stage, leading to substantial reductions in token usage and latency (TTFT) while maintaining high reasoning accuracy.

3.  **10 Most Important Citations:**

    1.  Guo et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. (arXiv:2501.12948)
        *   This paper introduces the DeepSeek-R1 model, whose distilled versions are the base LRMs that MinD builds upon and improves.
    2.  DeepSeek-AI et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.
        *   This specific reference [4] points to the distilled DeepSeek-R1 models (e.g., DeepSeek-R1-Distill-Qwen) used in MinD's experiments.
    3.  Wei et al. 2022. Chain-of-thought prompting elicits reasoning in large language models.
        *   This is the foundational paper that introduced Chain-of-Thought (CoT) prompting, the verbose reasoning paradigm that MinD aims to make more efficient and structured.
    4.  Shao et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models.
        *   This paper demonstrates a highly successful application of Group Relative Policy Optimization (GRPO) for mathematical reasoning, which is a core RL technique adapted and leveraged by MinD for reducing reasoning turns.
    5.  Lightman et al. 2023. Let's verify step by step. (arXiv:2305.20050)
        *   This paper introduces the MATH dataset, a crucial benchmark for complex mathematical reasoning, used extensively for training and evaluating MinD.
    6.  Cobbe et al. 2021. Training verifiers to solve math word problems. (arXiv:2110.14168)
        *   This paper introduced the GSM8K dataset, which is used as part of the training data for the SFT phase of MinD.
    7.  OpenAI et al. 2024. Gpt-4 technical report.
        *   Models from the GPT-4 lineage (specifically GPT-4o as mentioned in the MinD paper) are used to rephrase existing CoT traces into the multi-turn format required for MinD's SFT phase.
    8.  Hou et al. 2025. Thinkprune: Pruning long chain-of-thought of llms via reinforcement learning.
        *   This work represents a significant baseline method for improving reasoning efficiency by pruning CoTs, against which MinD's performance is compared.
    9.  Fu et al. 2025. Reasoning without self-doubt: More efficient chain-of-thought through certainty probing.
        *   This paper introduces Dynasor, another relevant baseline focused on making CoT more efficient, used for comparison with MinD.
    10. Yang et al. 2025. Dynamic early exit in reasoning models.
        *   This work, DEER, proposes a method for dynamic early exiting in reasoning and serves as an important baseline for evaluating MinD's token reduction and overall efficiency.
