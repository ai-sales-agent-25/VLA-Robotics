https://arxiv.org/abs/2505.11274

SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning

**1. Summary and Rating**

This paper introduces SelfBudgeter, a novel strategy to address the "overthinking" problem in large language models (LLMs), where models expend excessive computational resources (tokens) even for simple queries. SelfBudgeter enables LLMs to self-adaptively estimate the required token budget based on query difficulty and then generate a response adhering to this budget. The method employs a dual-phase training paradigm: a "Cold-Start" supervised fine-tuning (SFT) phase where the model learns to output an estimated token budget before the solution, followed by a reinforcement learning (RL) phase using a budget-guided Global Reward Policy Optimization (GRPO) algorithm. The reward function in the RL phase is designed to balance answer correctness, minimal token budget, and consistency between the allocated budget and the actual response length. SelfBudgeter also allows users to anticipate generation time from the model's budget estimate or to specify a budget manually. Experimental results on GSM8K and MATH benchmarks show that SelfBudgeter can significantly compress response length (e.g., up to 74.47% on MATH) while largely maintaining or even slightly improving accuracy compared to baseline models.

**Rating: 8.5/10**

For a PhD-level audience, this paper presents a well-motivated and timely contribution to the field of efficient LLM reasoning. The core idea of enabling the model to dynamically self-allocate a token budget is a valuable step beyond static or prompt-based length controls. The dual-phase training approach (SFT for format learning, RL for optimization) is methodologically sound, and the custom reward function thoughtfully considers multiple objectives (accuracy, brevity, adherence). The experimental validation on challenging benchmarks demonstrates significant practical benefits in terms of token reduction with minimal impact on accuracy, which is a key concern in LLM deployment. The added user control and transparency regarding generation time are also positive aspects. The paper is clearly written and the results are compelling. While the approach is promising, further investigation into its generalizability across diverse tasks and model architectures, as well as deeper analysis of the learned budgeting heuristics, would strengthen its impact. The work effectively tackles an important problem with a novel and practical solution.

**2. Main Ideas Discussed**

1.  **Self-Adaptive Token Budgeting:** The core concept is that the LLM learns to pre-estimate the necessary token budget for a given query based on its complexity before generating the actual reasoning steps and solution. This allows for dynamic allocation of computational resources, tailored to each specific input.
2.  **Dual-Phase Training for Budget-Aware Reasoning:** The paper proposes a specific training framework consisting of:
    *   A *Cold-Start phase* (SFT) to teach the model the desired output format, which includes explicitly stating a token budget (e.g., within `<budget>` tags) prior to the solution.
    *   An *RL training phase* (using GRPO) that optimizes the model's budgeting and reasoning capabilities using a reward function that incentivizes correctness, adherence to the allocated budget, and minimization of the budget itself for correct answers.
3.  **Controllable and Transparent Reasoning Length:** SelfBudgeter provides transparency by outputting its estimated budget, allowing users to anticipate generation time. Furthermore, it offers controllability by enabling users to pre-fill the token budget, forcing the model to generate a response within that specific constraint, thus improving interaction efficiency.

**3. 10 Most Important Citations**

1.  Aggarwal et al. 2025. L1: Controlling how long a reasoning model thinks with reinforcement learning. This paper introduces a direct competitor method (L1-Max) that also uses reinforcement learning to control the reasoning length of LLMs, serving as a key baseline in SelfBudgeter's experiments. [https://arxiv.org/abs/2503.04697](https://arxiv.org/abs/2503.04697)
2.  DeepSeek-AI et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. This paper describes a powerful reasoning model (Deepseek-R1) whose architecture and training methodologies likely influenced the base model used and the context of advanced LLM reasoning. [https://arxiv.org/abs/2501.12948](https://arxiv.org/abs/2501.12948)
3.  OpenAI. 2024. Learning to reason with llms. This work (referred to as O1) highlights significant advancements in LLM reasoning capabilities, setting a benchmark for complex reasoning tasks that SelfBudgeter aims to make more efficient. [https://openai.com/index/learning-to-reason-with-llms/](https://openai.com/index/learning-to-reason-with-llms/)
4.  Chen et al. 2024. Do not think that much for 2+3=? on the overthinking of ol-like llms. This paper directly addresses and quantifies the "overthinking" phenomenon in LLMs, which is the primary problem SelfBudgeter aims to solve. [https://arxiv.org/abs/2412.21187](https://arxiv.org/abs/2412.21187)
5.  Cobbe et al. 2021. Training verifiers to solve math word problems. This paper introduces the GSM8K dataset, a widely used benchmark for evaluating mathematical reasoning in LLMs and a key dataset in SelfBudgeter's experiments. [https://arxiv.org/abs/2110.14168](https://arxiv.org/abs/2110.14168)
6.  Hendrycks et al. 2021. Measuring mathematical problem solving with the MATH dataset. This paper introduces the MATH dataset, another crucial and challenging benchmark for mathematical problem-solving used to evaluate SelfBudgeter's performance, particularly its compression capabilities on complex tasks. [https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html)
7.  Wei et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. This foundational paper introduced Chain-of-Thought (CoT) prompting, highlighting the importance of explicit reasoning steps, the length and efficiency of which SelfBudgeter aims to optimize. [http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html)
8.  Han et al. 2024. Token-budget-aware llm reasoning. This work (TALE) is cited for introducing the concept of a token budget for LLM reasoning, which is a central theme expanded upon by SelfBudgeter's adaptive allocation mechanism. [https://arxiv.org/abs/2412.18547](https://arxiv.org/abs/2412.18547)
9.  Yang et al. 2025. Towards thinking-optimal scaling of test-time compute for llm reasoning. This paper (TOPS) attempts to enable models to autonomously determine the required computational effort, sharing a similar goal with SelfBudgeter in optimizing reasoning efficiency. [https://arxiv.org/abs/2502.18080](https://arxiv.org/abs/2502.18080)
10. Qwen. 2024. Qwq: Reflect deeply on the boundaries of the unknown. This describes another state-of-the-art reasoning model (QwQ), exemplifying the type of advanced LLMs whose efficiency SelfBudgeter seeks to improve. [https://qwenlm.github.io/blog/qwq-32b-preview/](https://qwenlm.github.io/blog/qwq-32b-preview/)
