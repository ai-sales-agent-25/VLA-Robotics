https://arxiv.org/abs/2505.11225

HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization

1.  **Brief Summary and Rating:**

    This paper introduces History-Aware Policy Optimization (HAPO), a novel method for training Large Language Models (LLMs) to generate more concise reasoning steps without significant loss of accuracy. The core problem HAPO addresses is the tendency of LLMs, especially when scaled for complex reasoning, to produce overly verbose outputs ("overthinking"), which increases inference costs and computational overhead. Unlike prior approaches that use universal budget constraints or query-level optimization without long-term memory of problem-specific progress, HAPO maintains a "history state" for each problem. This state tracks the minimum length of previously generated *correct* responses for that specific problem. HAPO then employs a novel reward function in a reinforcement learning setup. This function incentivizes the discovery of correct solutions that are shorter than the historical best for that problem, while also being designed to avoid overly penalizing shorter, incorrect responses, thus facilitating exploration. The method combines this length-based reward with a correctness reward. The authors demonstrate HAPO's effectiveness by training several open-source LLMs (DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, Qwen-2.5-1.5B-Instruct) on math benchmarks (GSM8K, MATH500, AIME2024). Results show substantial reductions in response length (33-59%) with only minor drops in accuracy (2-5%), achieving a better length-accuracy trade-off compared to existing baselines.

    **Rating: 9/10**

    For a PhD-level audience, this paper is rated highly. Its primary strength lies in the novel and intuitive mechanism of incorporating problem-specific historical performance (conciseness of correct solutions) directly into the RL reward structure. This "history-aware" component is a clever way to encourage progressive optimization towards conciseness for individual problems, a nuance often missed by global or batch-local optimization strategies. The paper clearly articulates the "overthinking" problem in LLMs and proposes a well-reasoned solution. The experimental validation is solid, using relevant benchmarks and comparing against appropriate baselines, including an ablated version of HAPO itself. The reported results, showing significant length reductions with minimal accuracy impact, are compelling and address a practical and important challenge in deploying LLMs. The methodology is clearly explained, particularly with Figure 2 illustrating the HAPO reward and history update. The provision of code also enhances its contribution. The work opens up interesting avenues for more adaptive and efficient LLM training.

2.  **Main Ideas Discussed:**

    1.  **History-Aware Policy Optimization (HAPO):** The central idea is a training paradigm that leverages historical information about the model's own past performance on specific problems. For each problem, HAPO tracks the minimum length of previously generated correct solutions (the history state, *hᵢ*). This history is used to dynamically adjust the reward signal during reinforcement learning, encouraging the model to find even more concise correct solutions over time.
    2.  **Dynamic Length-Based Reward Function:** HAPO introduces a novel reward function that has two main components: correctness and length. Crucially, the length reward is not static or globally defined but is relative to the historical best length (*hᵢ*) for the current problem. Correct responses shorter than *hᵢ* receive a positive length reward, while longer correct responses are penalized. The design also aims to avoid harshly penalizing shorter *incorrect* responses to encourage exploration towards more efficient solutions.
    3.  **Balancing Reasoning Accuracy with Conciseness:** The paper demonstrates that by jointly optimizing for correctness and this history-aware conciseness, LLMs can significantly reduce the verbosity and length of their reasoning chains (leading to lower inference costs) while largely maintaining their problem-solving accuracy. This achieves a more favorable trade-off compared to methods that only focus on correctness or use less adaptive length control mechanisms.

3.  **10 Most Important Citations:**

    1.  Aggarwal et al. 2025. L1: Controlling how long a reasoning model thinks with reinforcement learning. This citation is a key baseline method (L1-Exact, L1-Max) representing universal budget forcing, against which HAPO's performance is directly compared. [https://arxiv.org/abs/2503.04697](https://arxiv.org/abs/2503.04697)
    2.  Arora et al. 2025. Training language models to reason efficiently. This paper introduces Query-Opt, a query-level optimization baseline that HAPO is compared against, which rewards shorter responses within a training batch for the same query. [https://arxiv.org/abs/2502.04463](https://arxiv.org/abs/2502.04463)
    3.  DeepSeek-AI et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. This paper introduces DeepSeek-R1, one of the base language models that HAPO is applied to and evaluated on, demonstrating HAPO's ability to improve existing reasoning models. [https://arxiv.org/abs/2501.12948](https://arxiv.org/abs/2501.12948)
    4.  Wei et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. This foundational paper introduces chain-of-thought (CoT) prompting, which improves LLM reasoning but can lead to the verbosity HAPO aims to mitigate. [https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf)
    5.  Shao et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. This paper is relevant as it discusses advanced mathematical reasoning in LLMs and likely relates to the GRPO algorithm (mentioned as their algorithm [27]) used by HAPO for reinforcement learning training. [https://arxiv.org/abs/2402.03300](https://arxiv.org/abs/2402.03300) (The paper text cites this as the source for GRPO algorithm)
    6.  Sui et al. 2025. Stop overthinking: A survey on efficient reasoning for large language models. This survey defines and discusses the "overthinking" problem (verbose and unnecessary reasoning) in LLMs, which is the primary issue HAPO seeks to address. [https://arxiv.org/abs/2503.16419](https://arxiv.org/abs/2503.16419)
    7.  Luo et al. 2025. Deepscaler: Surpassing 01-preview with a 1.5b model by scaling rl. This work introduces DeepScaleR-1.5B-Preview, another base model used for HAPO experiments, and the DeepScaleR-Preview-Dataset used for training, showing HAPO's applicability to models already fine-tuned for reasoning. [https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-01-Preview-with-a-1-5B-Model-by-Scaling-RL-%19681902c1468005bed8ca303013a4e2](https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-01-Preview-with-a-1-5B-Model-by-Scaling-RL-%19681902c1468005bed8ca303013a4e2)
    8.  Cobbe et al. 2021. Training verifiers to solve math word problems. This paper introduces the GSM8K dataset, a key benchmark used in HAPO's evaluation to assess the model's reasoning and conciseness on grade-school math problems. [https://arxiv.org/abs/2110.14168](https://arxiv.org/abs/2110.14168)
    9.  Muennighoff et al. 2025. s1: Simple test-time scaling. This paper is cited as an example of test-time scaling and methods that control length, such as by inserting special tokens, contrasting with HAPO's training-time, history-aware approach. [https://arxiv.org/abs/2501.19393](https://arxiv.org/abs/2501.19393)
    10. Yeo et al. 2025. Demystifying long chain-of-thought reasoning in llms. This work is referenced for its analysis of long CoT and its universal penalty functions, providing context for length control strategies, and is also explored in HAPO's appendix for adding a repetition penalty. [https://arxiv.org/abs/2502.03373](https://arxiv.org/abs/2502.03373)
