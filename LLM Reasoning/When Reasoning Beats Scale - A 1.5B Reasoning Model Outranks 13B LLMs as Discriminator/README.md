https://arxiv.org/abs/2505.03786

**When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator**

1.  **Brief Summary and Rating:**

    This paper investigates the performance of a distilled 1.5B parameter reasoning model (DeepSeek-R1, referred to as Distill-R1) against several larger, state-of-the-art non-reasoning LLMs within a generator-discriminator framework specifically for the text-to-SQL task. The authors introduce a novel method for extracting soft scores from the chain-of-thought (CoT) outputs of reasoning models to enable more nuanced candidate ranking. Their central hypothesis is that reasoning models are more effective discriminators.

    The results demonstrate that the 1.5B Distill-R1 significantly outperforms larger non-reasoning models like CodeLlama-7B and CodeLlama-13B in its role as a discriminator, achieving notably higher F1 scores for classification and better overall discrimination and execution accuracy in the end-to-end planning pipeline. The study also finds that there are limits to the benefits of increased compute budget (token limits for CoT) or additional contextual information (like database schema) for reasoning models, with performance gains diminishing or even reversing beyond certain thresholds, and output redundancy increasing. A key finding is that while Distill-R1 excels as a discriminator, it underperforms as a generator, even compared to smaller non-reasoning LLMs. This suggests that for reasoning models, unlike non-reasoning ones, generation is more challenging than discrimination. The work highlights the potential of smaller reasoning models as specialized, highly effective discriminators in agentic LLM planning infrastructures.

    **Rating: 9/10**
    This is a well-executed and insightful paper. It addresses a pertinent question regarding the optimal roles of emerging reasoning models versus scaled non-reasoning models in practical agentic frameworks. The introduction of a soft-scoring mechanism for CoT outputs in a discrimination context is a valuable methodological contribution. The findings are clearly presented and robustly supported by empirical evidence, offering significant implications for designing efficient and effective LLM-based planning systems. The paper's focused investigation and clear, actionable conclusions make it a strong contribution to the field.

2.  **Main Ideas Discussed:**

    1.  **Superior Discrimination by Smaller Reasoning Models:** A compact 1.5B parameter reasoning model (Distill-R1) can significantly outperform much larger non-reasoning LLMs (e.g., CodeLlama-7B and 13B) when used as a discriminator in a text-to-SQL planning framework. This is attributed to its inherent reasoning capabilities, which allow for more effective evaluation of candidate SQL queries.
    2.  **Inherent Limitations and Diminishing Returns in Reasoning Models:** The discrimination performance of reasoning models does not linearly scale with increased inference-time compute budget (CoT length) or additional contextual information. Beyond a certain point, more compute or context yields marginal gains or can even degrade performance and increase output redundancy, indicating limitations in their logical depth or ability to utilize extensive information effectively.
    3.  **Asymmetric Performance in Generation vs. Discrimination for Reasoning Models:** Unlike many non-reasoning LLMs where discrimination can be as or more challenging than generation, the studied reasoning model (Distill-R1) performs substantially better as a discriminator than as a generator. As a generator, it is outperformed even by smaller non-reasoning models, suggesting its optimal deployment in planning architectures is in an evaluative rather than a generative role.

3.  **10 Most Important Citations:**

    1.  Chen et al. 2024. When is tree search useful for llm planning? it depends on the discriminator. This citation is crucial as it establishes the generator-discriminator LLM planning framework that the current paper adopts and builds upon for its experimental setup.
    2.  Wei et al. 2023. Chain-of-thought prompting elicits reasoning in large language models. This work is fundamental as it details Chain-of-Thought (CoT) prompting, the core mechanism enabling the reasoning capabilities of the DeepSeek-R1 model evaluated in the paper. (Note: The paper also cites a 2022 version by Wei et al. with the same title, but [34] is explicitly mentioned with CoT in the abstract).
    3.  DeepSeek AI Team et al. 2025. Deepseek-r1-distill-qwen-1.5b: Compact reasoning model for mathematical and coding tasks. This paper describes the specific distilled 1.5B parameter reasoning model (Distill-R1) that is the primary subject of investigation in this study.
    4.  Yu et al. 2018. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task. This paper introduces the Spider dataset, which is the benchmark used for the text-to-SQL task evaluation in the current study.
    5.  Rozière et al. 2024. Code llama: Open foundation models for code. This work presents CodeLlama, a series of state-of-the-art non-reasoning LLMs (7B and 13B variants) used as key comparison points against the reasoning model.
    6.  Gu et al. 2023. Don't generate, discriminate: A proposal for grounding language models to real-world environments. This paper's proposal aligns with the current study's findings that reasoning models might be better suited for discrimination than generation, drawing parallels to human cognitive strengths.
    7.  West et al. 2024. The generative AI paradox: “what it can create, it may not understand". This work offers a relevant counterpoint by hypothesizing that generative models (often non-reasoning) may lack discrimination abilities commensurate with their generation skills, contrasting with the reasoning model's strengths.
    8.  Zeng et al. 2025. Revisiting the test-time scaling of o1-like models: Do they truly possess test-time scaling capabilities?. This citation is relevant for its discussion of an inverse scaling phenomenon with CoT length, which supports the current paper's findings on diminishing returns from increased compute budgets for reasoning.
    9.  Yao et al. 2023. Tree of thoughts: Deliberate problem solving with large language models. This paper introduces an advanced LLM planning and problem-solving framework, providing important context for the broader field of LLM-based structured planning that the current research contributes to.
    10. Yao et al. 2023. React: Synergizing reasoning and acting in language models. This work details a framework for LLMs to combine reasoning and acting, pertinent to the agentic systems and LLM planning infrastructures discussed in the paper.
