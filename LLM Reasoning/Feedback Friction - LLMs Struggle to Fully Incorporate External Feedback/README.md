https://huggingface.co/papers/2506.11930

FEEDBACK FRICTION: LLMs Struggle to Fully Incorporate External Feedback

### 1. Summary and Rating

**Summary:**

This paper systematically investigates the upper bounds of Large Language Model (LLM) performance improvement when given external feedback. The authors design a controlled experimental framework where a "solver" LLM attempts a problem, and if it fails, a powerful "feedback generator" LLM (with access to the ground truth) provides targeted, high-quality feedback. This iterative process is repeated up to 10 times. The central finding, termed **"Feedback Friction,"** is that even under these near-ideal conditions, state-of-the-art models like Claude 3.7 and the Llama-4 family consistently plateau in performance, failing to reach the theoretical accuracy they could achieve by perfectly incorporating the provided feedback. The study demonstrates this limitation across a diverse set of tasks, including math, knowledge, and scientific reasoning. An error analysis reveals that this failure is primarily due to the models' **"feedback resistance"**—a fundamental inability to integrate the corrective information—rather than deficiencies in the feedback itself. The authors test and rule out several potential causes, such as model overconfidence and data familiarity, and show that mitigation strategies like advanced sampling offer only modest improvements, highlighting that Feedback Friction is a deep-seated and challenging limitation of current LLMs.

**Rating: 9/10**

This is an excellent empirical paper that identifies, names, and rigorously characterizes a significant and previously underexplored limitation in LLMs. The experimental design is strong, carefully decoupling feedback generation from the solver's capabilities to create a controlled environment for studying feedback incorporation. The use of a wide array of state-of-the-art models and challenging benchmarks lends significant weight to the findings. The primary contribution—the concept of "Feedback Friction"—is a valuable framing for a critical problem in the pursuit of reliable, self-improving AI systems. The subsequent analysis, which systematically rules out several intuitive explanations (confidence, familiarity, etc.), is thorough and adds depth to the work by narrowing the scope for future investigation. While the paper does not offer a definitive mechanistic cause or a complete solution, its strength lies in the clarity and robustness with which it establishes the problem, providing a solid foundation and a clear direction for future research in this area.

### 2. Main Ideas

1.  **"Feedback Friction" as a Fundamental Limitation:** The core idea is that even when provided with repeated, high-quality, and near-perfect external feedback, LLMs exhibit a fundamental resistance to correction. They consistently fail to fully integrate this feedback, causing their performance to plateau well below the theoretical maximum accuracy they could achieve. This phenomenon is demonstrated across various SOTA models and a diverse range of complex reasoning tasks.
2.  **"Feedback Resistance" is the Dominant Failure Mode:** The paper performs an error analysis on persistent failures and finds that the primary cause is not poor feedback quality but the model's inability to act on correct guidance. This "feedback resistance" suggests an internal, structural limitation in how models update their reasoning pathways or overcome strong initial priors, even when explicitly told how they are wrong.
3.  **Simple Explanations and Mitigations are Insufficient:** The authors investigate and largely rule out several plausible causes for this friction, including model overconfidence, familiarity with entities in the training data, and problem complexity. Furthermore, they show that while mitigation techniques like temperature sampling combined with rejecting past incorrect answers can yield some improvements, they do not resolve the core issue, underscoring the deep-seated nature of feedback friction.

### 3. Top 10 Most Important Citations

1.  **Huang et al. (2023).** Large language models cannot self-correct reasoning yet. This paper establishes the key premise that LLMs struggle with self-correction without external guidance, motivating this work's investigation into the limits of improvement even *with* near-perfect external guidance. [https://arxiv.org/abs/2310.01798](https://arxiv.org/abs/2310.01798)
2.  **Shinn et al. (2023).** Reflexion: an autonomous agent with dynamic memory and self-reflection. This is a foundational paper on iterative self-improvement through feedback, providing a framework that this paper builds upon and critically evaluates the limits of. [https://arxiv.org/abs/2303.11366](https://arxiv.org/abs/2303.11366)
3.  **Madaan et al. (2023).** Self-refine: Iterative refinement with self-feedback. This influential work demonstrates that models can improve by generating their own feedback, and the current paper contrasts this by using a stronger, external feedback source to probe the absolute limits of refinement. [https://arxiv.org/abs/2303.17651](https://arxiv.org/abs/2303.17651)
4.  **Tyen et al. (2023).** Llms cannot find reasoning errors, but can correct them! This work posits that LLMs are better at correcting errors than finding them, a premise that "Feedback Friction" challenges by showing there are hard limits to this corrective ability. [https://arxiv.org/abs/2310.01798](https://arxiv.org/abs/2310.01798)
5.  **Pan et al. (2023).** Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. This survey provides the broad context of the self-correction field in which the paper's contribution of "Feedback Friction" is situated. [https://arxiv.org/abs/2308.03188](https://arxiv.org/abs/2308.03188)
6.  **Wu et al. (2024).** Clasheval: Quantifying the tug-of-war between an llm's internal prior and external evidence. This paper is highly relevant as it explores the conflict between a model's internal knowledge and external information, providing a conceptual underpinning for why "Feedback Friction" might occur. [https://arxiv.org/abs/2404.10198](https://arxiv.org/abs/2404.10198)
7.  **Luo et al. (2023).** An empirical study of catastrophic forgetting in large language models during continual fine-tuning. This citation represents the broader "elasticity-plasticity dilemma," contextualizing feedback resistance as a related phenomenon where models struggle to integrate new information without disrupting existing knowledge. [https://arxiv.org/abs/2308.08747](https://arxiv.org/abs/2308.08747)
8.  **Hendrycks et al. (2020).** Measuring massive multitask language understanding. As the origin of the MMLU benchmark, this citation is fundamental to the paper's experimental validation, as MMLU and MMLU-Pro are core datasets used to demonstrate Feedback Friction.
9.  **Mallen et al. (2023).** When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories. This paper provides the PopQA dataset and the concept of data familiarity, which the authors use to test and rule out as a primary cause of Feedback Friction. [https://arxiv.org/abs/2212.10511](https://arxiv.org/abs/2212.10511)
10. **Jiang et al. (2024).** Self-[in]correct: Llms struggle with discriminating self-generated responses. This prior work from some of the same authors highlights LLMs' difficulty in evaluating their own outputs, setting the stage for the current paper's focus on what happens when a reliable external evaluator provides the feedback instead. [https://arxiv.org/abs/2404.04298](https://arxiv.org/abs/2404.04298)
