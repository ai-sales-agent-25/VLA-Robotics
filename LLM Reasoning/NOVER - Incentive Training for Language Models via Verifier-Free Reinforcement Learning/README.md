https://arxiv.org/abs/2505.16022

NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning

**1. Summary and Rating:**

This paper introduces NOVER (NO-VERifier Reinforcement Learning), a verifier-free reinforcement learning framework designed for incentive training of language models. NOVER addresses a key limitation of existing incentive training methods, which typically rely on external verifiers (e.g., for math or code), by utilizing the model's own reasoning perplexity on ground-truth answers as a reward proxy. This approach requires only standard supervised fine-tuning (SFT) data. The framework incorporates a policy-proxy synchronization mechanism, where the policy model itself serves as the proxy, and a composite reward function that considers reasoning quality (via perplexity rank), efficiency (shorter, effective reasoning), and format adherence. These elements aim to ensure stable training and mitigate reward hacking. NOVER demonstrates strong performance, outperforming a model of the same size distilled from the significantly larger DeepSeek R1 671B by 7.7% on average, and generalizes incentive training to a wide array of text-to-text tasks. Furthermore, its flexibility enables novel applications such as inverse incentive training, where the model learns to generate outputs (e.g., stories) that satisfy given rubrics.

**Rating: 9/10**

This paper presents a significant advancement in incentive training for LLMs by proposing a well-reasoned verifier-free approach (NOVER) that broadens applicability beyond domains with readily available verifiers. The method of using perplexity as a self-supervised reward signal, combined with policy-proxy synchronization and a carefully designed composite reward, is innovative and effectively addresses potential issues like the "curse of proxy" and reward hacking. The reported empirical results, including outperforming distilled versions of much larger models and enabling novel training paradigms like inverse incentive training, are compelling. The work is methodologically sound, clearly presented, and tackles a practical limitation in the field, making a strong contribution to improving LLM reasoning capabilities and alignment with minimal human supervision for the RL phase.

**2. Main Ideas Discussed:**

1.  **Verifier-Free Incentive Training via Reasoning Perplexity Reward:** The core idea of NOVER is to enable incentive training for language models without relying on external verifiers. It achieves this by calculating rewards based on the perplexity of the ground-truth answer, conditioned on the model's generated intermediate reasoning steps. This perplexity, computed using a proxy model (which is a synchronized version of the policy model itself), serves as an intrinsic signal of reasoning quality, allowing the method to be applied to diverse text-to-text tasks where explicit verifiers are hard or costly to construct.
2.  **Stable and Efficient Reinforcement Learning through Policy-Proxy Synchronization and Composite Rewards:** NOVER ensures training stability and promotes effective reasoning by synchronizing the policy model with its proxy counterpart and employing a multi-component reward function. This reward combines a discretized reasoning reward (based on perplexity rank), an efficiency reward (favoring shorter, high-quality reasoning), and a format adherence reward. This composite structure, coupled with the Group Relative Policy Optimization (GRPO) algorithm, helps mitigate reward hacking and guides the model towards generating high-quality and efficient reasoning.

**3. 10 Most Important Citations:**

1.  **Guo et al. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.** This paper introduces DeepSeek R1-Zero and the concept of Reinforcement Learning with Verifiable Reward (RLVR), establishing the "incentive training" paradigm that NOVER directly builds upon and aims to generalize by removing the verifier dependency. (Link: `CoRR, abs/2501.12948`)
2.  **Shao et al. 2024. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.** This citation is for GRPO (Group Relative Policy Optimization), the specific RL algorithm used in NOVER for policy updates. (Although the title is DeepSeekMath, GRPO is detailed in works from this group, and the paper cites GRPO (Shao et al., 2024) on page 3). (Link: `CoRR, abs/2402.03300` which is for DeepSeekMath; GRPO is likely from associated work or within this line of research).
3.  **Christiano et al. 2017. Deep Reinforcement Learning from Human Preferences.** This foundational work on RLHF is relevant as NOVER offers an alternative to training explicit reward models, and it's cited in the context of the "curse of proxy," a common challenge in RL with learned or proxy rewards.
4.  **Wei et al. 2022. Chain-of-thought Prompting Elicits Reasoning in Large Language Models.** This paper introduced Chain-of-Thought prompting, a key technique for improving LLM reasoning, which incentive training methods like NOVER aim to instill more robustly and spontaneously.
5.  **Hu et al. 2022. LoRA: Low-rank Adaptation of Large Language Models.** This paper introduces LoRA, the parameter-efficient fine-tuning technique used by NOVER for both the policy and proxy models, making the training process more computationally feasible. (Link: `OpenReview.net`)
6.  **Gorbatovski et al. 2025. Learn Your Reference Model for Real Good Alignment.** NOVER adopts the exponential smoothing technique from this paper (cited as TR-DPO) for its policy-proxy synchronization. (Link: `OpenReview.net`)
7.  **Gurung and Lapata, 2025. Learning to Reason for Long-form Story Generation.** This work is cited as an example of using an extra frozen model as a proxy and calculating relative perplexity gain, highlighting an alternative approach that NOVER contrasts with by using the policy model itself as the proxy and employing group normalization. (Link: `arXiv preprint arXiv:2503.22828`)
8.  **Ma et al. 2025. General-Reasoner: Advancing LLM Reasoning Across All Domains.** This citation is important as it represents efforts to build general-purpose verifier models (which NOVER circumvents) and provides the WebInstruct dataset and its official verifier model (GV) against which NOVER is compared. (Link: `https://github.com/TIGER-AI-Lab/General-Reasoner/blob/main/General_Reasoner.pdf`)
9.  **Su et al. 2025. Crossing the Reward Bridge: Expanding RL with Verifiable Rewards Across Diverse Domains.** This paper supports NOVER's "Efficiency Reward" component by suggesting that effective reasoning length is crucial, and it also explores extending verifiable rewards to diverse domains. (Link: `CoRR, abs/2503.23829`)
10. **Amodei et al. 2016. Concrete Problems in AI Safety.** This paper is cited for introducing the "curse of proxy," a fundamental challenge where optimizing a proxy reward leads to exploiting its flaws rather than achieving the true objective, which NOVER aims to mitigate through its reward design and synchronization. (Link: `CoRR, abs/1606.06565`)
