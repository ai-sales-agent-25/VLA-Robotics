https://arxiv.org/abs/2505.09655

DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models

1.  **Brief Summary and Rating:**
    This paper addresses a limitation in Group Relative Policy Optimization (GRPO), a reinforcement learning technique for post-training large language models (LLMs), particularly in low-resource settings. The authors identify a "diversity-quality inconsistency" where GRPO's reliance on scalar, solution-level rewards fails to distinguish between semantically diverse reasoning paths, even if they lead to similar outcomes. To mitigate this, they propose Diversity-aware Reward Adjustment (DRA), a method that incorporates semantic diversity into reward computation. DRA utilizes Submodular Mutual Information (SMI), specifically a Graph-Cut function over embedding similarities, to downweight redundant completions and amplify rewards for diverse ones. This encourages better exploration during training while maintaining exploitation of high-quality samples. The proposed DRA integrates with GRPO and its variant DR. GRPO, resulting in DRA-GRPO and DRA-DR. GRPO. Evaluations on five mathematical reasoning benchmarks show that DRA-DR. GRPO achieves state-of-the-art performance using only 7,000 fine-tuning samples, outperforming baselines and demonstrating effectiveness in low-resource scenarios.

    **Rating: 8.5/10**
    The paper makes a valuable and well-motivated contribution by explicitly addressing the lack of diversity consideration in GRPO's reward mechanism. The proposed DRA method, leveraging SMI, is a theoretically sound and principled approach to encourage exploration of diverse reasoning paths. The empirical results are strong, particularly the state-of-the-art performance in a low-resource setting (small model, few samples), which is a significant practical advantage. The "diversity-quality inconsistency" is well-articulated and empirically supported. The integration with existing GRPO frameworks is seamless. The paper could be slightly strengthened by a more in-depth exploration of different SMI instantiations or a broader set of tasks beyond mathematical reasoning, but for its focused scope, it presents a compelling and efficient solution to an important problem in LLM fine-tuning.

2.  **Main Ideas Discussed:**
    *   **Diversity-Quality Inconsistency in GRPO:** The paper highlights that standard GRPO and its variants, while effective, primarily use scalar reward signals (e.g., correctness) that do not capture the semantic diversity of different reasoning paths in LLM completions. This means that distinct, valuable reasoning approaches might receive similar or identical rewards, hindering the model's ability to explore and learn a richer variety of problem-solving strategies. The authors empirically demonstrate this inconsistency by showing a lack of correlation between reward differences and semantic distances of completions.
    *   **Diversity-aware Reward Adjustment (DRA) using Submodular Mutual Information (SMI):** To address the inconsistency, the core proposal is DRA, a method to reweight the rewards of sampled completions based on their semantic diversity within a group. DRA employs SMI (instantiated with a Graph-Cut function over embedding similarities) to quantify the novelty or redundancy of each completion relative to others in the same batch. Completions that are semantically distinct from others receive higher adjusted rewards, while redundant ones are downweighted. This mechanism is designed to promote exploration of diverse reasoning paths during RL training.
    *   **Efficient and Effective Low-Resource LLM Fine-tuning:** A key outcome of integrating DRA with GRPO (DRA-GRPO and DRA-DR. GRPO) is its ability to achieve strong performance, even state-of-the-art on mathematical reasoning benchmarks, using significantly limited resources. The paper emphasizes training a relatively small model (1.5B parameters) with only 7,000 samples and a low computational cost, showcasing the practical utility of the method for scenarios where extensive data or computational power is unavailable.

3.  **List of 10 Most Important Citations:**
    1.  Shao et al. 2024. (Implicitly, GRPO algorithm as employed in DeepSeek-AI, 2025). *The paper doesn't list Shao et al. 2024 directly in the references, but GRPO is foundational. The closest explicit reference to GRPO's application is DeepSeek-AI, 2025.* This citation is crucial as it introduces the Group Relative Policy Optimization (GRPO) algorithm, which is the base RL method DRA-GRPO aims to improve.
    2.  DeepSeek-AI. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. This paper showcases the R1-Zero training framework and the successful application of GRPO, which DRA-GRPO builds upon and refines. *(Note: There are two DeepSeek-AI 2025 entries; this one is also listed as Guo et al. 2025 later, referring to the same work.)*
    3.  Liu et al. 2025. Understanding r1-zero-like training: A critical perspective. arXiv preprint arXiv:2503.20783. This work introduces DR. GRPO, a variant of GRPO, with which the proposed DRA method is also integrated and evaluated, making it an important comparative baseline and integration target.
    4.  Iyer et al. 2021a. Submodular combinatorial information measures with applications in machine learning. In Algorithmic Learning Theory, pages 722-754. PMLR. This citation (and Iyer et al. 2021b) is fundamental as it details Submodular Mutual Information (SMI), the core mathematical tool used in DRA to quantify and promote diversity in rewards.
    5.  Iyer et al. 2021b. Generalized submodular information measures: Theoretical properties, examples, optimization algorithms, and applications. IEEE Transactions on Information Theory, 68(2):752-781. This provides further theoretical background and applications of SMI, underpinning the principled nature of the diversity measurement in DRA.
    6.  Dang and Ngo. 2025. Reinforcement learning for reasoning in small llms: What works and what doesn't. arXiv preprint arXiv:2503.16219. This paper provides the high-quality, low-resource training dataset used for fine-tuning and is a key reference for the low-resource experimental setup and baselines.
    7.  Guo et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. This is the R1-Zero training framework paper that utilizes GRPO, providing the context and baseline for the proposed DRA-GRPO method's improvements. *(This is the same work as DeepSeek-AI 2025, listed with authors)*
    8.  Günther et al. 2023. Jina embeddings 2: 8192-token general-purpose text embeddings for long documents. Preprint, arXiv:2310.19923. This paper introduces the embedding model (jina-embeddings-v2-small-en) used to compute semantic similarities between completions, which is crucial for the DRA mechanism.
    9.  Luo et al. 2025. Deepscaler: Surpassing ol-preview with a 1.5b model by scaling rl. https://github.com/agentica-project/deepscaler. Github. This work provides a strong baseline (DeepScaleR-1.5B-Preview) and dataset source, against which the proposed method demonstrates competitive or superior performance with fewer resources.
    10. He et al. 2024. OlympiadBench: A challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3828–3850, Bangkok, Thailand. Association for Computational Linguistics. This is one of the key mathematical reasoning benchmarks (OlympiadBench) used for evaluating the performance of DRA-GRPO.
