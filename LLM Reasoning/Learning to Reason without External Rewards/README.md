https://huggingface.co/papers/2505.19590

Learning to Reason without External Rewards

**1. Summary and Rating**

This paper introduces Reinforcement Learning from Internal Feedback (RLIF), a novel paradigm for training large language models (LLMs) to improve their reasoning capabilities without relying on external rewards or labeled data. The authors propose INTUITOR, a specific RLIF method that utilizes the model's own "self-certainty" as its sole intrinsic reward signal. Self-certainty is defined as the average KL divergence between the model's output token distribution and a uniform distribution. INTUITOR integrates this self-certainty score into the Group Relative Policy Optimization (GRPO) framework, replacing external verifiable rewards.

Experiments conducted primarily on mathematical reasoning (MATH dataset) and evaluated on tasks like GSM8K, MATH500, and out-of-domain code generation (LiveCodeBench, CRUXEval) demonstrate INTUITOR's effectiveness. The results show that INTUITOR can match the performance of GRPO (which uses external rewards) on in-domain mathematical benchmarks. More notably, INTUITOR exhibits superior generalization to out-of-domain tasks, such as code generation, even when trained exclusively on mathematical problems. The paper argues that this approach offers a scalable alternative for developing autonomous AI systems, especially in scenarios where verifiable rewards are scarce or unavailable, and promotes structured reasoning and better instruction-following. The authors also show that online self-certainty helps prevent reward exploitation issues seen with static reward models.

**Rating: 9/10**

This paper presents a compelling and timely contribution to the field of LLM training. The concept of RLIF, and specifically INTUITOR's use of self-certainty, is innovative and addresses significant limitations of existing RLHF and RLVR methods, namely their reliance on costly human supervision or domain-specific verifiers. The empirical results are strong, particularly the demonstration of out-of-domain generalization from math to code using only intrinsic signals. The ablation study on KL penalty and the analysis of online self-certainty preventing reward hacking add to the paper's rigor. The potential for LLMs to self-improve based on an internal sense of confidence is a significant step towards more autonomous and scalable AI. A point could be potentially gained with even broader domain explorations or deeper theoretical backing for self-certainty as a universally optimal intrinsic reward, but the current work is already a very strong and well-executed piece of research.

**2. Main Ideas Discussed**

1.  **Reinforcement Learning from Internal Feedback (RLIF):** The core conceptual contribution is the introduction of RLIF as a new paradigm for LLM training. This framework proposes that LLMs can improve their capabilities, particularly reasoning, by optimizing for intrinsic signals generated by the model itself, without needing external rewards, human labels, or domain-specific verifiers. This shifts the focus from external validation to internal self-assessment.
2.  **INTUITOR using Self-Certainty as Intrinsic Reward:** The paper proposes INTUITOR, a concrete instantiation of RLIF. INTUITOR's key mechanism is the use of "self-certainty" (the average KL divergence between the model's output distribution and a uniform distribution) as the sole reward signal. The hypothesis, supported by experiments, is that by reinforcing outputs for which the model is more certain, the LLM learns to produce more coherent, well-reasoned, and ultimately more correct responses, even generalizing this improved reasoning process to new domains.
3.  **Superior Out-of-Domain Generalization and Autonomous Learning:** A major finding is that INTUITOR, despite being trained without external rewards, not only matches supervised methods like GRPO on in-domain tasks (e.g., math) but also demonstrates significantly better generalization to out-of-domain tasks (e.g., code generation). This suggests that learning from intrinsic signals can foster a more fundamental understanding or robust reasoning process that transfers across domains, paving the way for more autonomous and scalable LLM self-improvement.

**3. 10 Most Important Citations**

1.  **Shao et al. 2024.** Deepseekmath: Pushing the limits of mathematical reasoning in open language models. (The paper actually cites "GRPO [Shao et al., 2024]" in text, but the reference list points to Deepseekmath by Shao et al. for related mathematical reasoning, and another Shao et al. for GRPO is not explicitly listed. However, GRPO is fundamental. Assuming there might be a slight citation mismatch or the GRPO work is closely tied/by same authors as a DeepMind paper, if the explicit GRPO paper is "Shao, P., Rao, A., Korbak, M., ... & Beattie, C. (2024). Group Relative Policy Optimization." it would be more direct. Given the text, the concept of GRPO is critical.)
    *   **Relation:** INTUITOR directly replaces the external reward signal in the Group Relative Policy Optimization (GRPO) algorithm with its intrinsic self-certainty scores, making GRPO a foundational algorithmic framework for this work.
2.  **Kang et al. 2025.** Scalable best-of-n selection for large language models via self-certainty. arXiv preprint arXiv:2502.18581.
    *   **Relation:** This paper introduces and validates the "self-certainty" metric that INTUITOR adopts as its sole intrinsic reward signal, making it a cornerstone of the proposed method.
3.  **Ouyang et al. 2022.** Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730–27744.
    *   **Relation:** This is a foundational paper for RLHF, a dominant paradigm for aligning LLMs, whose limitations (cost, bias) motivate the exploration of alternative frameworks like RLIF.
4.  **Guo et al. 2025.** Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948.
    *   **Relation:** This paper presents DeepSeek-R1, a state-of-the-art model trained with RLVR, which serves as a key example of the verifiable reward approaches that INTUITOR aims to provide an alternative to, particularly in terms of not needing gold solutions. INTUITOR is benchmarked against GRPO, which is used in such models.
5.  **Hendrycks et al. 2021.** Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874.
    *   **Relation:** The MATH dataset is the primary dataset used for training INTUITOR and for its in-domain evaluation, making this citation crucial for understanding the experimental context.
6.  **Gao et al. 2023.** Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 10835–10866. PMLR.
    *   **Relation:** This work highlights a critical failure mode ("reward hacking") in RLHF/RLVR where models overfit to the reward signal, an issue that INTUITOR attempts to mitigate by using an online, evolving intrinsic reward (self-certainty).
7.  **Williams. 1992.** Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229–256.
    *   **Relation:** REINFORCE is a fundamental policy gradient algorithm mentioned as a simpler alternative for implementing RLVR/RLIF, providing context for the class of RL algorithms used.
8.  **Schulman et al. 2017.** Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.
    *   **Relation:** PPO is another widely used policy optimization algorithm in RL, and GRPO (used by INTUITOR) is related to this family of algorithms, providing broader context for the RL methodology.
9.  **Zelikman et al. 2022.** Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476–15488.
    *   **Relation:** STaR is an important prior work in self-improvement for LLMs that bootstraps reasoning capabilities, often relying on outcome evaluation, contrasting with INTUITOR's process-oriented intrinsic reward.
10. **Oudeyer and Kaplan. 2007.** What is intrinsic motivation? a typology of computational approaches. Frontiers in neurorobotics, 1:108.
    *   **Relation:** This paper provides a foundational perspective on intrinsic motivation in AI, which is the broader conceptual umbrella under which RLIF and self-certainty-driven learning fall, aiming for more autonomous learning.
