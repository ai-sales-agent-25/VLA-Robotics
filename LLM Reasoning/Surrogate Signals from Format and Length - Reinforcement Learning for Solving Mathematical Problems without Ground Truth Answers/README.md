https://huggingface.co/papers/2505.19439

**Paper Name:** Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers

**1. Summary and Rating**

This paper introduces a novel reinforcement learning (RL) approach for training Large Language Models (LLMs) to solve mathematical problems without requiring ground truth answers. The core idea is to use "surrogate signals" derived from the format and length of the generated solutions as rewards within the Group Relative Policy Optimization (GRPO) framework. The authors demonstrate that a reward function based solely on format correctness can achieve performance comparable to standard GRPO (which uses ground truth) in the early training phases, primarily by teaching the model structural presentation. By subsequently incorporating a carefully designed length-based reward, which encourages comprehensive yet concise reasoning, their "Format-Length GRPO" method can match and even surpass the performance of ground-truth-reliant GRPO on mathematical reasoning benchmarks, notably achieving 40.0% accuracy on AIME2024 with a 7B base model. The study posits that this label-free RL approach primarily "unlocks" the latent mathematical reasoning capabilities already present in pre-trained LLMs by guiding them towards better answering habits and reasoning depth, rather than teaching new factual knowledge.

**Rating: 9/10**

This paper presents a highly innovative and practical solution to a significant challenge in training LLMs for specialized tasks like mathematical reasoningâ€”the dependency on expensive and often scarce ground truth data. The methodology is well-motivated, and the use of format and length as surrogate signals is elegantly simple yet proven effective through comprehensive experiments. The analysis of training dynamics, including the distinct roles of format and length rewards over time and their impact on response characteristics (e.g., use of reflective words), offers valuable insights into the RL fine-tuning process for LLMs. The finding that such methods can "unlock" existing capabilities rather than purely instilling new knowledge is a noteworthy contribution to the understanding of LLM adaptation. The reported state-of-the-art results on AIME2024 using a 7B model without ground truth answers underscore the potential impact of this work. The paper is well-structured and the claims are strongly supported by empirical evidence. A minor point of potential confusion is the textual reference to "Shao et al. (2024)" for GRPO, which does not appear to have a corresponding entry in the provided bibliography; however, this does not detract from the core contributions regarding the surrogate signals.

**2. Main Ideas Discussed**

1.  **Format and Length as Effective Surrogate Reward Signals:** The central idea is that format correctness and appropriate response length can serve as powerful proxy signals for actual answer correctness in mathematical problem-solving. Format rewards help the LLM learn the structural conventions of solutions quickly, while length rewards guide the model to produce responses that are neither too terse (lacking reasoning) nor too verbose (containing redundancy or incorrect derivations), thereby encouraging more effective reasoning.
2.  **RL "Unlocks" Latent LLM Capabilities:** The paper argues that reinforcement learning with these surrogate signals does not primarily teach the LLM new mathematical knowledge but rather activates and refines its pre-existing, latent reasoning abilities. The model already possesses the foundational skills from pre-training; the RL process helps it learn how to effectively structure and express solutions, akin to developing "good answering habits." This is supported by pass@N experiments showing comparable latent performance across differently trained models.
3.  **Dual-Phase Learning Dynamics:** The training process with format-length rewards exhibits distinct phases. Initially, the model prioritizes learning the correct format, leading to a reduction in response length as it prunes redundant information. Subsequently, the length reward mechanism encourages the model to expand its responses strategically, fostering deeper and more complex reasoning paths (evidenced by increased use of reflective keywords) without excessive verbosity, ultimately improving accuracy on more difficult problems.

**3. 10 Most Important Citations**

1.  DeepSeek-AI et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. This paper is highly relevant as it also explores large-scale RL for math and coding tasks, including the use of format rewards and analysis of response length, providing a key benchmark and conceptual parallel for the current work. URL https://arxiv.org/abs/2501.12948
2.  Schulman et al. 2017. Proximal policy optimization algorithms. This paper introduces PPO, a foundational RL algorithm that is mentioned as a common technique for LLM optimization and upon which GRPO (used in this work) builds. URL https://arxiv.org/abs/1707.06347
3.  Hendrycks et al. 2021. Measuring mathematical problem solving with the math dataset. This citation is crucial as the MATH dataset is a key benchmark used for training and evaluating the models in the paper, establishing a standard for mathematical reasoning assessment. URL https://arxiv.org/abs/2103.03874
4.  Brown et al. 2020. Language models are few-shot learners. This is a seminal paper on GPT-3, highlighting the capabilities of large pre-trained models that form the basis for the LLMs fine-tuned in this research.
5.  Luo et al. 2025. Deepscaler: Surpassing o1-preview with a 1.5b model by scaling rl. The DeepScaleR dataset, derived from this work, is one of the two primary training datasets used in the paper's experiments. URL https://oatllm.notion.site/oat-zero (Note: The paper cites this as "Notion Blog", the actual arXiv for DeepScaleR might be different or this refers to a specific version/dataset release associated with the blog post, DeepScaleR is mentioned with Luo et al. as from DeepSeek). The URL given in the paper for Luo et al. (2025) is actually for Liu et al. (2025a). The correct DeepScaleR paper seems to be "Luo, Michael, et al. "Deepscaler: Surpassing o1-preview with a 1.5 b model by scaling rl." arXiv preprint arXiv:2401.10056 (2024)." if this is the one, but sticking to the provided reference list: Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with a 1.5b model by scaling rl, 2025. Notion Blog. The DeepScaleR dataset is one of the primary training datasets used in the paper's experiments. (No direct ArXiv link given for this specific entry in the paper's bibliography).
6.  Liu et al. 2025b. Understanding r1-zero-like training: A critical perspective. This paper is cited for comparative results (Oat-Zero-7B) and as part of the related work on RL-based training without direct answer supervision. URL https://arxiv.org/abs/2503.20783
7.  Chen et al. 2025. An empirical study on eliciting and improving r1-like reasoning models. This work is referenced in the discussion of length-based rewards, specifically for their proposal of a linear length reward which the current paper contrasts with its own approach. URL https://arxiv.org/abs/2503.04548
8.  Yeo et al. 2025. Demystifying long chain-of-thought reasoning in llms. This citation is relevant for its exploration of length-based rewards and its applicability in label-free settings, providing context for the current paper's length reward design. URL https://arxiv.org/abs/2502.03373
9.  Zeng et al. 2025. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. This paper (SimpleRL-Zero-7B) is cited as a related work providing benchmark results for comparison in the context of reinforcement learning without ground truth. URL https://arxiv.org/abs/2503.18892
10. Ouyang et al. 2022. Training language models to follow instructions with human feedback. This paper is a foundational work on RLHF (Reinforcement Learning from Human Feedback), a key technique for aligning LLMs, and provides broader context for using RL to adapt pre-trained models. URL https://arxiv.org/abs/2203.02155
