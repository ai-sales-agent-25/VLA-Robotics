https://arxiv.org/abs/2505.16410

Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning

**1. Summary and Rating**

**Summary:**
This paper introduces Tool-Star, a reinforcement learning (RL)-based framework designed to enable Large Language Models (LLMs) to autonomously invoke and coordinate multiple external tools for complex, stepwise reasoning. The authors address the challenge of effective multi-tool collaborative reasoning, which remains underdeveloped compared to single-tool or non-RL approaches. Tool-Star features two main components: 
First, a general **Tool-Integrated Reasoning Data Synthesis Pipeline** to address the scarcity of tool-use data. This pipeline automatically generates tool-use trajectories by combining tool-integrated prompting with hint-based sampling, followed by quality normalization and a difficulty-aware classification process to create a curriculum from easy to hard samples.
Second, a **two-stage training framework** to enhance multi-tool collaboration. This includes: (1) a cold-start supervised fine-tuning (SFT) stage to provide LLMs with initial experience in reasoning patterns using tool-invocation feedback, and (2) a multi-tool self-critic RL algorithm with a hierarchical reward design. This RL stage reinforces understanding of complex reward structures (including rewards for effective multi-tool collaboration) and promotes efficient tool use, further improved by interleaving self-critic reward fine-tuning.
The framework integrates six types of tools (three for training: Search Engine, Web Browser Agent, Code Interpreter; and three for inference-time optimization: Code Debugger, Tool-Use Backtracer, Reasoning Chain Refiner). The authors conduct experiments on over 10 challenging reasoning benchmarks, demonstrating that Tool-Star significantly improves both the effectiveness and efficiency of tool-augmented reasoning in LLMs compared to various baselines.

**Rating for a PhD-level audience: 8.5/10**

This paper tackles a significant and timely problem in LLM reasoning: enabling effective and collaborative use of multiple tools. The proposed Tool-Star framework is comprehensive, addressing key challenges from data scarcity (via a sophisticated synthesis pipeline) to nuanced learning of multi-tool coordination (via a two-stage SFT+RL approach with hierarchical rewards and self-criticism).
The methodological contributions, particularly the data synthesis strategy and the multi-tool self-critic RL algorithm with hierarchical rewards, appear robust and well-motivated for the problem. The systematic design for both data and training is a strength. The paper's empirical evaluation across a diverse set of over 10 challenging benchmarks, showing superior performance, lends credibility to its claims. The focus on not just correctness but also rationality and efficiency of tool use, and multi-tool collaboration, is commendable.
The complexity of the system is considerable, but it seems justified by the multifaceted nature of the problem. For a PhD-level audience, the detailed methodology and the ambition of tackling multi-tool collaboration in a principled way would be appreciated. The paper seems to build upon and extend existing lines of research in RL for LLMs and tool integration in a novel combination. Minor points that might prevent a higher score without further scrutiny of the full experimental details (not fully available here) would be the potential for hyperparameter sensitivity in such a complex system and ensuring the novelty of individual sub-components is clearly delineated against the vast related work. However, the overall framework and its empirical validation appear strong.

**2. Main Ideas Discussed**

The main ideas discussed in this paper are:

1.  **A Comprehensive RL-based Framework (Tool-Star) for Multi-Tool Collaborative Reasoning:** The core contribution is Tool-Star, a system designed to empower LLMs to autonomously select, invoke, and coordinate multiple external tools (like search engines, code interpreters, web browsers) during stepwise reasoning to solve complex problems. This framework emphasizes moving beyond single-tool usage to effective collaboration between different tools.
2.  **Scalable Tool-Use Data Synthesis Pipeline:** To overcome the scarcity of high-quality data for training LLMs to use tools, the paper proposes a general pipeline for synthesizing tool-use trajectories. This involves tool-integrated prompting, hint-based sampling (inspired by START [28]) to diversify tool invocation patterns, followed by quality normalization (e.g., frequency control, duplicate removal) and a difficulty-aware classification to organize data for curriculum learning.
3.  **Two-Stage Training Strategy for Effective Tool Collaboration:** Tool-Star employs a progressive training approach. It starts with **Cold-Start Supervised Fine-Tuning (SFT)** on simpler, synthesized data to initialize the LLM's ability to follow tool invocation patterns. This is followed by a **Multi-Tool Self-Critic Reinforcement Learning (RL)** algorithm. This RL stage uses a hierarchical reward mechanism that explicitly rewards not only correct answers and tool formats but also effective multi-tool collaboration. It also incorporates a self-critic component to help the LLM internalize these complex reward principles.

**3. 10 Most Important Citations**

Here is a list of 10 important citations, formatted as requested:

1.  OpenAI et al. 2024. Learning to reason with llms. This paper is cited as an example of advanced LLMs exhibiting diverse emergent behaviors in reasoning, setting the context for the capabilities Tool-Star aims to enhance with tools. (No link provided in the paper for this citation entry)
2.  DeepSeek-AI et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. This work is highlighted as an advanced model demonstrating remarkable reasoning capabilities empowered by large-scale RL, relevant to Tool-Star's RL-centric approach. (No link provided in the paper for this citation entry)
3.  Gou et al. 2024. Tora: A tool-integrated reasoning agent for mathematical problem solving. This paper represents an SFT-based method for tool-integrated reasoning, a category of approaches Tool-Star contrasts with and aims to improve upon with its RL methodology. (No link provided in the paper for this citation entry)
4.  Li et al. 2025. START: self-taught reasoner with tools. Tool-Star's hint-based sampling strategy in its data synthesis pipeline is directly built upon the START method proposed in this paper. (No link provided in the paper for this citation entry)
5.  Shao et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. This paper is significant as Tool-Star adopts the Group Relative Policy Optimization (GRPO) from this work as its RL algorithm, and it's a key paper in mathematical reasoning. (No link provided in the paper for this citation entry)
6.  Song et al. 2025. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. This citation is an example of RL-based methods focusing on a single tool (search), which helps contextualize Tool-Star's contribution towards multi-tool collaboration. (No link provided in the paper for this citation entry)
7.  Wang et al. 2025. Otc: Optimal tool calls via reinforcement learning. This is mentioned as recent initial progress in multi-tool collaborative reasoning systems, highlighting the research gap that Tool-Star addresses more systematically. (No link provided in the paper for this citation entry)
8.  Lightman et al. 2024. Let's verify step by step. This paper introduced the MATH500 dataset, one of the challenging computational reasoning benchmarks used to evaluate Tool-Star's performance. (No link provided in the paper for this citation entry)
9.  Cobbe et al. 2021. Training verifiers to solve math word problems. This work is relevant for its discussion of outcome-based rewards, a concept pertinent to designing reward functions in RL for reasoning tasks, which Tool-Star extends with its hierarchical reward design. (No link provided in the paper for this citation entry)
10. Paranjape et al. 2023. ART: automatic multi-step reasoning and tool-use for large language models. This is cited as an early and notable approach in Tool-Integrated Reasoning (TIR), demonstrating LLMs interacting with external tools, providing foundational context for Tool-Star. (No link provided in the paper for this citation entry)

====
====

**Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning**

**1. Brief Summary and Rating:**

This paper introduces Tool-Star, a novel reinforcement learning (RL) framework designed to enable Large Language Models (LLMs) to autonomously and collaboratively use multiple external tools during stepwise reasoning. The authors address two main challenges: the scarcity of tool-use data and the difficulty of training LLMs for effective multi-tool collaboration.

To tackle data scarcity, Tool-Star proposes a general tool-integrated reasoning data synthesis pipeline. This pipeline combines tool-integrated prompting with hint-based sampling to generate diverse tool-use trajectories. These trajectories then undergo quality normalization (filtering low-quality samples, removing duplicates, standardizing formats) and difficulty-aware classification (organizing data from easy to hard for curriculum learning).

For training, Tool-Star employs a two-stage framework. First, a "cold-start" supervised fine-tuning (SFT) stage uses the synthesized easy-to-medium difficulty data to provide the LLM with an initial understanding of tool invocation and reasoning patterns. Second, a "multi-tool self-critic RL algorithm" further refines the LLM's capabilities using harder data. This RL stage features a hierarchical reward design that not only assesses answer correctness and tool-use format but also explicitly rewards effective multi-tool collaboration (e.g., using both search and code interpreter). The self-critic mechanism involves the LLM generating multiple responses, with a rule-based reward function assigning scores, and then fine-tuning the LLM using a DPO objective on these self-generated preference pairs. The framework also incorporates inference-time tools like a code debugger, tool-use backtracer, and reasoning chain refiner to improve robustness.

The authors evaluate Tool-Star on over 10 challenging reasoning benchmarks, including computational tasks (AIME, MATH500, GSM8K) and knowledge-intensive QA (WebWalker, HotpotQA). Results show that Tool-Star significantly outperforms existing prompting-based, SFT-based, and single-tool RL-based methods in both effectiveness and tool-use efficiency, demonstrating its ability to handle multi-tool collaborative reasoning.

**Rating: 9/10**

This paper presents a comprehensive and well-thought-out solution to a significant problem in the LLM agent space: effective multi-tool reasoning. The data synthesis pipeline is a practical contribution to address a common bottleneck. The two-stage training, particularly the multi-tool self-critic RL with hierarchical rewards, is a sophisticated approach to guide the LLM towards complex collaborative behaviors. The experimental validation is thorough across diverse benchmarks, and the ablation studies support the design choices. The inclusion of inference-time tools further enhances the practical applicability. The work is novel, technically sound, and addresses a timely research direction with significant potential impact. A minor point could be further exploration of the scalability of the self-critic DPO training, but overall, this is a very strong contribution.

**2. Main Ideas Discussed:**

1.  **Automated Data Synthesis for Multi-Tool Reasoning:** The paper introduces a scalable "Tool-Integrated Reasoning Data Synthesis Pipeline" to create high-quality training data for LLMs using tools. This pipeline combines tool-integrated prompting and hint-based sampling to generate diverse tool-use trajectories, followed by quality normalization and difficulty-aware classification to structure the data for effective curriculum learning. This addresses the critical issue of data scarcity for training tool-augmented LLMs.
2.  **Two-Stage Training for Multi-Tool Collaboration via Reinforcement Learning:** Tool-Star proposes a progressive training framework. It starts with "Cold-Start Supervised Fine-Tuning" on simpler data to teach basic tool invocation. This is followed by a "Multi-Tool Self-Critic Reinforcement Learning Algorithm" which uses a hierarchical reward mechanism. This reward system is crucial as it not only focuses on task success and correct tool format but explicitly incentivizes the collaborative use of multiple distinct tools (e.g., search and code interpreter) within a single reasoning process, pushing beyond single-tool optimization. The self-critic component further refines the model's understanding of these complex rewards.
3.  **Enhanced Robustness and Efficiency in Tool-Augmented Reasoning:** The framework is designed not just for performance but also for rational and efficient tool usage. The data synthesis filters overly frequent tool calls, and the RL stage aims to learn when tools are genuinely necessary. Furthermore, inference-time mechanisms like a code debugger, tool-use backtracer, and reasoning chain refiner are introduced to handle common failure modes (e.g., code errors, failed tool calls, context overflow), making the tool-use process more robust and practical.

**3. List of 10 Most Important Citations:**

1.  **Shao et al. 2024.** Deepseekmath: Pushing the limits of mathematical reasoning in open language models. *This citation (referred to as [51] GRPO in the paper) introduces Group Relative Policy Optimization (GRPO), the RL algorithm adopted by Tool-Star for its policy optimization, highlighting its relevance for complex reasoning tasks.*
2.  **Li et al. 2025.** START: self-taught reasoner with tools. *This citation ([28] in the paper) is important as Tool-Star's hint-based sampling proposes two hint instructions (Logical Verification and Answer Reflection) following the START method for diversifying tool invocation patterns in data synthesis.*
3.  **Gou et al. 2024.** Tora: A tool-integrated reasoning agent for mathematical problem solving. *This paper ([17]) presents TORA, a tool-integrated reasoning agent whose code interpreter design is followed by Tool-Star for its own code execution tool, ensuring secure and accurate execution.*
4.  **Li et al. 2025.** Torl: Scaling tool-integrated RL. *This work ([35]) is a key baseline (ToRL) representing code-enhanced single-tool RL methods, which Tool-Star compares against and aims to improve upon, particularly in multi-tool scenarios.*
5.  **Jin et al. 2025.** Search-r1: Training llms to reason and leverage search engines with reinforcement learning. *This citation ([23]) introduces Search-R1, another important single-tool (search-enhanced) RL baseline that Tool-Star is compared against, demonstrating the limitations of single-tool specialization that Tool-Star overcomes.*
6.  **Li et al. 2025.** Search-o1: Agentic search-enhanced large reasoning models. *This paper ([31]) introduces Search-01, a reasoning-model-driven RAG method used as a baseline. Tool-Star builds upon the idea of LLMs autonomously deciding tool use but extends it to multi-tool collaboration and RL-based learning.*
7.  **Dong et al. 2024.** How abilities in large language models are affected by supervised fine-tuning data composition. *This citation ([11]) is relevant as it discusses curriculum learning and data composition for SFT, a concept utilized in Tool-Star's difficulty-aware data classification and cold-start fine-tuning stage.*
8.  **OpenAI. 2024.** Learning to reason with llms, September 2024. *This citation ([41]) represents the state-of-the-art in LLM reasoning capabilities, often achieved through RL, providing context and motivation for Tool-Star's endeavor to enhance these capabilities further through multi-tool integration.*
9.  **DeepSeek-AI et al. 2025.** Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. *Similar to OpenAI's work, this paper ([9]) demonstrates the power of RL in enhancing LLM reasoning, setting a high bar and context for research like Tool-Star.*
10. **Cobbe et al. 2021.** Training verifiers to solve math word problems. *This paper ([4, 5]) introduces the concept of outcome-based rewards (verifiers) for mathematical problems, a principle related to how Tool-Star evaluates answer correctness in its reward mechanism.*
