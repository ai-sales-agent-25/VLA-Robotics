https://arxiv.org/abs/2502.19645

Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success

### 1. Summary and Rating

This paper conducts a systematic empirical study of different strategies for fine-tuning pre-trained vision-language-action models (VLAs) for novel robotic tasks. Using OpenVLA as the base model, the authors investigate three key design choices: action decoding schemes (autoregressive vs. parallel), action representations (discrete vs. continuous), and learning objectives (next-token prediction vs. L1 regression vs. diffusion).

Based on their findings, they propose an **Optimized Fine-Tuning (OFT)** recipe that combines parallel decoding with action chunking, a continuous action representation, and a simple L1 regression objective. This recipe, instantiated as **OpenVLA-OFT**, is shown to dramatically improve both inference speed and task performance. On the LIBERO simulation benchmark, OpenVLA-OFT increases the success rate from 76.5% to 97.1% and boosts action generation throughput by 26x compared to the original fine-tuning approach. In challenging real-world evaluations on a bimanual ALOHA robot, an augmented version (OFT+) outperforms other state-of-the-art VLAs and imitation learning policies on dexterous, high-frequency tasks. The work demonstrates that a well-designed fine-tuning recipe can be more critical than pretraining data coverage and allows existing VLAs to be adapted effectively to new robot platforms and tasks with significant gains in speed and success.

**Rating: 9/10**

This is an excellent paper that addresses a crucial, practical problem in robotics: how to best adapt large pre-trained models. The empirical analysis is thorough, well-structured, and provides clear, actionable insights. The authors systematically dissect the fine-tuning process and demonstrate that a combination of relatively simple techniques leads to state-of-the-art performance while drastically improving inference efficiency. The comparison to more complex diffusion-based methods highlights the power of algorithmic simplicity. The validation in both simulation and complex, real-world bimanual tasks lends strong credibility to the proposed OFT recipe, making this work highly valuable to the robotics and machine learning communities.

### 2. Main Ideas

1.  **Parallel Decoding and Action Chunks are Superior for Speed and Performance:** The paper's core insight is that moving away from the standard autoregressive, token-by-token action generation is highly beneficial. By implementing parallel decoding (generating all action dimensions for a future chunk simultaneously), they achieve a massive 26x-43x speedup. Crucially, this does not come at the cost of performance; in fact, success rates improve significantly, likely because action chunking helps capture temporal dependencies and reduces compounding errors.
2.  **Simple L1 Regression on Continuous Actions is Sufficient and Efficient:** The authors compare different learning objectives and find that a simple L1 regression loss on continuous actions performs on par with more complex and computationally expensive diffusion-based models in terms of final task success. However, L1 regression offers much faster training convergence and significantly lower inference latency, making it a more practical choice for real-world robotics applications.
3.  **Effective Fine-Tuning Can Outweigh Pretraining Data Mismatch:** The OpenVLA model was pre-trained exclusively on single-arm robot data. Despite this, when adapted using the proposed OFT recipe, it was able to match or outperform newer VLA models (RDT-1B and πo) that were pre-trained on extensive bimanual data. This strongly suggests that the adaptation methodology itself is a critical component for success, potentially even more so than the specific coverage of the pretraining dataset. For tasks with critical language components, they further show that incorporating FiLM (Feature-wise Linear Modulation) is essential to ensure robust language grounding.

### 3. Important Citations

1.  **Kim et al. (2024). Openvla: An open-source vision-language-action model.** This paper introduces OpenVLA, the base model that is systematically analyzed and improved upon through the proposed fine-tuning recipe. [https://arxiv.org/abs/2406.09246](https://arxiv.org/abs/2406.09246)
2.  **Brohan et al. (2023). Rt-2: Vision-language-action models transfer web knowledge to robotic control.** This is a foundational VLA paper that serves as a key inspiration, demonstrating how to adapt large vision-language models to robotics. [https://arxiv.org/abs/2307.15818](https://arxiv.org/abs/2307.15818)
3.  **Black et al. (2024). pi0: A vision-language-action flow model for general robot control.** This paper introduces πo, a state-of-the-art diffusion-based VLA that serves as a primary, high-performance baseline for the real-world bimanual robot experiments. [https://arxiv.org/abs/2410.24164](https://arxiv.org/abs/2410.24164)
4.  **Chi et al. (2023). Diffusion policy: Visuomotor policy learning via action diffusion.** This work introduces Diffusion Policy, representing the diffusion-based learning paradigm that the authors compare their simpler L1 regression objective against.
5.  **Liu et al. (2024). Libero: Benchmarking knowledge transfer for lifelong robot learning.** This paper provides the LIBERO simulation benchmark, the main environment used for controlled experiments on task performance and inference efficiency.
6.  **Zhao et al. (2023). Learning fine-grained bimanual manipulation with low-cost hardware.** This work introduces the ALOHA robot system and the Action Chunking with Transformers (ACT) model, which are used for the real-world experiments and serve as an important baseline. [https://arxiv.org/abs/2304.13705](https://arxiv.org/abs/2304.13705)
7.  **Perez et al. (2018). Film: Visual reasoning with a general conditioning layer.** This paper introduces Feature-wise Linear Modulation (FiLM), the technique the authors integrate into their OFT recipe to improve language grounding for the real-world ALOHA tasks.
8.  **Liu et al. (2024). Rdt-1b: a diffusion foundation model for bimanual manipulation.** This paper presents RDT-1B, another state-of-the-art diffusion-based VLA that is used as a key competitor and baseline in the real-world bimanual manipulation experiments. [https://arxiv.org/abs/2410.07864](https://arxiv.org/abs/2410.07864)
9.  **O'Neill et al. (2023). Open x-embodiment: Robotic learning datasets and rt-x models.** This paper presents the large-scale Open X-Embodiment dataset which was used to pre-train the base OpenVLA model. [https://arxiv.org/abs/2310.08864](https://arxiv.org/abs/2310.08864)
10. **Hu et al. (2021). Lora: Low-rank adaptation of large language models.** This paper introduces Low-Rank Adaptation (LoRA), the parameter-efficient fine-tuning method used throughout this work to adapt OpenVLA. [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)
