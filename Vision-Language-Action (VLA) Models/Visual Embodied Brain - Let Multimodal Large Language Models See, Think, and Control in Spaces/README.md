https://arxiv.org/abs/2506.00123

Visual Embodied Brain: Let Multimodal Large Language Models See, Think, and Control in Spaces

1.  **Brief Summary and Rating:**
    This paper introduces the Visual Embodied Brain (VeBrain), a unified framework designed to enable Multimodal Large Language Models (MLLMs) to perform perception, visual-spatial reasoning, and real-world robotic control. The core innovation lies in reformulating robotic control tasks (keypoint detection and embodied skill recognition) into common text-based MLLM tasks within a 2D visual space. This approach aims to unify learning objectives and prevent the degradation of multimodal understanding capabilities often seen when MLLMs are adapted for robotic control. A novel robotic adapter is proposed to translate these textual control signals into motion policies for physical robots, forming a closed-loop system. Furthermore, the authors introduce VeBrain-600k, a large-scale, high-quality instruction dataset curated with human expertise and semi-automated data engines, featuring multimodal chain-of-thought (CoT) data to enhance compositional capabilities. Experiments demonstrate VeBrain's superior performance over existing MLLMs (like Qwen2.5-VL) and Vision-Language-Action (VLA) models across 13 multimodal benchmarks, 5 spatial intelligence benchmarks, and various robotic tasks on legged robots and robotic arms. The paper argues that VeBrain successfully balances multimodal understanding, spatial reasoning, and control, outperforming prior methods in achieving a versatile embodied agent.

    **Rating: 9/10**
    This paper addresses a significant and timely challenge in AI: extending the capabilities of MLLMs to embodied agents that can interact with the physical world. The proposed VeBrain framework offers a novel and principled approach to unifying perception, reasoning, and control by reformulating control tasks to align better with standard MLLM training. The introduction of the VeBrain-600k dataset, particularly its emphasis on CoT data, is a valuable contribution to the field. The comprehensive experimental validation across a wide range of benchmarks and robotic tasks robustly supports the claims of superior performance and adaptability. The work is well-motivated, the methodology is clearly presented, and the results are compelling. It represents a solid step towards more capable and general-purpose robotic intelligence. The slight deduction is for the inherent complexities in real-world robot deployment that might not be fully captured even in extensive simulations and controlled lab experiments, and the reliance on a pre-trained MLLM (Qwen2.5-VL) which, while practical, means the foundational MLLM's limitations could carry over.

2.  **Main Ideas Discussed:**

    *   **Unified Framework for Perception, Reasoning, and Control:** The central idea is the VeBrain framework itself, which aims to integrate multimodal understanding, visual-spatial reasoning, and robotic control into a single MLLM. This is achieved by reformulating robotic control (keypoint detection and embodied skill recognition) as text-based tasks in the 2D visual space, aligning them with the MLLM's native capabilities and mitigating the issue of catastrophic forgetting or task conflict often seen in VLA models.
    *   **Robotic Adapter and Closed-Loop Control:** A key component is the novel robotic adapter that translates the MLLM's textual/keypoint outputs into executable motion policies for real robots. This adapter, in conjunction with the MLLM, forms a closed-loop system, allowing for dynamic and robust control by handling real-time feedback and adjustments (e.g., dynamic takeover when the adapter fails).
    *   **High-Quality Data Engine (VeBrain-600k):** The paper introduces VeBrain-600k, a substantial instruction dataset (200k multimodal understanding, 312k visual-spatial reasoning, 88k robot control) to train VeBrain. A significant aspect of this dataset is the use of a semi-automated data engine and the incorporation of multimodal Chain-of-Thought (CoT) data, which is shown to be crucial for improving the model's compositional reasoning and complex task handling abilities.

3.  **10 Most Important Citations:**

    1.  Bai et al. 2025. Qwen2.5-vl technical report. This paper describes the Qwen2.5-VL model, which serves as the foundational pretrained MLLM backbone for VeBrain, indicating that VeBrain builds upon its architecture and capabilities. [No direct link in paper, refers to arXiv:2502.13923]
    2.  Brohan et al. 2023. Rt-2: Vision-language-action models transfer web knowledge to robotic control. This paper is a key VLA model (RT-2) that demonstrates transferring knowledge to robotic control, representing an important related work and baseline concept that VeBrain aims to improve upon in terms of preserving general MLLM capabilities.
    3.  Chen et al. 2024. Commonsense reasoning for legged robot adaptation with vision-language models. This citation (VLM-PC) is used as a baseline in ablation studies, particularly for its approach to MLLM-based robot control, highlighting a specific point of comparison for VeBrain's framework. [No direct link in paper, refers to arXiv:2407.02666]
    4.  Kim et al. 2024. Openvla: An open-source vision-language-action model. This is a recent open-source VLA model used as a strong baseline in robotic manipulation tasks, against which VeBrain demonstrates superior performance. [No direct link in paper, refers to arXiv:2406.09246]
    5.  Zhou et al. 2025. Chatvla: Unified multimodal understanding and robot control with vision-language-action model. This paper proposes another unified model for understanding and control, representing a direct competitor or related approach whose limitations (like compromised multimodal understanding) VeBrain aims to address. [No direct link in paper, refers to arXiv:2502.14420]
    6.  Qi et al. 2025. Gpt4scene: Understand 3d scenes from videos with vision-language models. This citation is a source for visual-spatial reasoning data in the VeBrain-600k dataset and a benchmark model, indicating VeBrain's engagement with advanced 3D scene understanding. [No direct link in paper, refers to arXiv:2501.01428]
    7.  Chen et al. 2024. Sharegpt4v: Improving large multi-modal models with better captions. This dataset is one of the open-source datasets used for collecting multimodal understanding data for VeBrain-600k, showing how VeBrain leverages existing high-quality data resources.
    8.  Liu et al. 2024. Mminstruct: A high-quality multi-modal instruction tuning dataset with extensive diversity. This is another source dataset for the multimodal understanding component of VeBrain-600k, emphasizing the diversity of data used for training VeBrain.
    9.  Dai et al. 2017. Scannet: Richly-annotated 3d reconstructions of indoor scenes. ScanNet is a foundational dataset for 3D scene understanding, used by VeBrain for generating visual-spatial reasoning data, highlighting the use of rich 3D environmental data.
    10. Black et al. 2024. Ï€0: A vision-language-action flow model for general robot control. This paper introduces another VLA model used as a strong baseline in the robot control experiments, especially for manipulation, serving as a benchmark for VeBrain's control capabilities. [No direct link in paper, refers to arXiv:2410.24164]
