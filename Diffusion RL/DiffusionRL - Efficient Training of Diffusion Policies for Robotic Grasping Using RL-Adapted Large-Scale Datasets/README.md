https://arxiv.org/abs/2505.18876

**DiffusionRL: Efficient Training of Diffusion Policies for Robotic Grasping Using RL-Adapted Large-Scale Datasets**

### 1. Summary and Rating

This paper presents "DiffusionRL," an end-to-end pipeline for efficiently training diffusion policies for complex dexterous grasping tasks. The core problem addressed is the data-hungriness of imitation learning models and the difficulty of adapting large, general-purpose datasets to specific robotic setups. The authors find that simply using grasps from a large-scale dataset like DexGraspNet is often insufficient, as the grasps may be unstable or not directly transferable.

Their proposed solution is to use a lightweight Reinforcement Learning (RL) agent to "enhance" the data from DexGraspNet. The RL agent, trained with the TD3 algorithm, learns to predict small, corrective actions that are added to the original grasp data, making the grasps more stable and successful within the target simulation environment. This refined, high-quality dataset is then used to train a compact diffusion policy. The authors demonstrate their approach by training a policy for a ShadowHand to grasp three different objects. The resulting policy achieves a high success rate of approximately 80% on grasping these objects in randomly generated poses, showcasing the method's ability to improve generalization without manual data collection.

**Rating: 8.5/10**

This paper presents a clever and pragmatic solution to a significant bottleneck in robotic learning: the acquisition of high-quality, task-specific demonstration data. The core contribution—using RL as a data-refinement tool rather than for end-to-end control—is an elegant way to leverage the strengths of both large-scale, pre-existing datasets and reinforcement learning. The methodology is sound, clearly articulated, and the results effectively demonstrate the pipeline's value. The work is a solid contribution to the field, though its experimental validation is limited to simulation and three object types. A stronger baseline comparison (e.g., policy performance on the un-refined dataset) would have further highlighted the impact of the RL enhancement stage.

### 2. Main Ideas Discussed

1.  **RL-Based Dataset Enhancement:** The central idea is to refine a large but imperfect dataset (DexGraspNet) using a targeted, lightweight RL agent. Instead of learning a complex policy from scratch, the RL agent learns a simple, corrective action policy that "fixes" suboptimal grasps from the dataset, adapting them to the specific robot embodiment and a dynamic range of poses. This transforms a noisy, general dataset into a high-quality, specialized one suitable for imitation learning.
2.  **Efficient Diffusion Policy Training via Data Refinement:** The paper demonstrates that by first improving the quality of the training data, a compact and effective diffusion policy can be trained for a complex, high-dimensional dexterous manipulation task. This approach circumvents the need for extensive manual data collection, which is a primary obstacle in robotics, and allows the diffusion model to learn a robust and generalizable policy, as evidenced by its high success rate on unseen object poses.
3.  **A Generalizable Pipeline for Policy Learning:** The authors frame their work as a complete pipeline that starts with a generic dataset and produces a capable, task-specific policy. This framework is proposed as a scalable method for adapting existing large-scale datasets to new robotic tasks and environments, significantly lowering the barrier to entry for training and deploying advanced generative policies like diffusion models in robotics.

### 3. 10 Most Important Citations

1.  **Chi et al. 2024. Diffusion policy: Visuomotor policy learning via action diffusion.** This is the foundational paper that introduced the "Diffusion Policy" framework, which is the core imitation learning method the authors implement and train using their novel data enhancement pipeline. [https://arxiv.org/abs/2303.04137](https://arxiv.org/abs/2303.04137)
2.  **Wang et al. 2023. Dexgraspnet: A large-scale robotic dexterous grasp dataset for general objects based on simulation.** This paper provides the DexGraspNet dataset, which is the large-scale but imperfect source of grasping data that this work aims to refine and adapt. [https://arxiv.org/abs/2210.02697](https://arxiv.org/abs/2210.02697)
3.  **Lillicrap et al. 2019. Continuous control with deep reinforcement learning.** This work is the basis for the Twin Delayed DDPG (TD3) algorithm used by the authors to train their lightweight RL agent for dataset enhancement. [https://arxiv.org/abs/1509.02971](https://arxiv.org/abs/1509.02971)
4.  **Ho et al. 2020. Denoising diffusion probabilistic models.** This is the seminal paper that introduced Denoising Diffusion Probabilistic Models (DDPMs), the underlying generative modeling technique that powers the diffusion policy. [https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239)
5.  **Tao et al. 2024. Maniskill3: Gpu parallelized robotics simulation and rendering for generalizable embodied ai.** This citation identifies the ManiSkill simulation environment where all experiments, including RL training and diffusion policy validation, were conducted. [https://arxiv.org/abs/2410.00425](https://arxiv.org/abs/2410.00425)
6.  **Ronneberger et al. 2015. U-net: Convolutional networks for biomedical image segmentation.** This paper introduced the U-Net architecture, which serves as the backbone for the conditional diffusion model used in this work to predict actions. [https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)
7.  **Belkhale et al. 2023. Data quality in imitation learning.** This reference validates the paper's core motivation by discussing the critical importance of high-quality data for successful imitation learning, a problem the DiffusionRL pipeline directly addresses. [https://arxiv.org/abs/2306.02437](https://arxiv.org/abs/2306.02437)
8.  **Wu et al. 2025. Graspgf: Learning score-based grasping primitive for human-assisting dexterous grasping.** This is cited as related work that also uses diffusion models for grasping and importantly notes the high percentage of inaccurate samples in DexGraspNet, reinforcing the problem statement of the current paper. [https://arxiv.org/abs/2309.06038](https://arxiv.org/abs/2309.06038)
9.  **Pan et al. 2024. Vision-language-action model and diffusion policy switching enables dexterous control of an anthropomorphic hand.** This paper is cited to highlight a potential application for DiffusionRL, as it could be used to efficiently generate the small, high-quality datasets needed for specialized auxiliary policies in more complex robotic systems. [https://arxiv.org/abs/2410.14022](https://arxiv.org/abs/2410.14022)
10. **Hua et al. 2021. Learning for a robot: Deep reinforcement learning, imitation learning, transfer learning.** This citation provides a broad overview of the key machine learning paradigms—Reinforcement Learning and Imitation Learning—that are synergistically combined in the paper's methodology. [https://doi.org/10.3390/s21041278](https://doi.org/10.3390/s21041278)
