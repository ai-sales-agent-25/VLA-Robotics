https://arxiv.org/abs/2507.05169

**Critiques of World Models**

### 1. Summary and Rating

This paper presents a comprehensive critique of current mainstream approaches to building "world models" for artificial intelligence, arguing that their primary goal should be to simulate all actionable possibilities for purposeful reasoning, rather than simply generating high-fidelity sensory data like video. The authors begin by framing the ideal world model through the lens of psychology's "hypothetical thinking" and science fiction, where an agent can simulate future outcomes to choose the best course of action.

The core of the paper is a systematic dissection of five key aspects of a prominent school of thought on world modeling, exemplified by Joint Embedding Predictive Architectures (JEPA). The authors argue:
1.  **Data:** Text is as crucial as sensory data due to its high information density and ability to encode abstract concepts, and a general-purpose model must use all modalities.
2.  **Representation:** A purely continuous representation is brittle. The authors advocate for a mixed discrete-continuous representation, where discrete tokens (as in LLMs) provide stability, compositionality, and abstraction.
3.  **Architecture:** They defend generative, autoregressive models, arguing that the critiqued non-generative approaches are still functionally autoregressive and that eliminating the generative decoder leads to ungrounded, unstable latent spaces.
4.  **Objective:** They demonstrate, with theoretical propositions, that training with a latent-space reconstruction loss is prone to collapse and that a generative loss, grounded in reconstructing observable data, provides a more robust learning signal.
5.  **Usage:** They contrast the limitations of Model Predictive Control (MPC) with the flexibility of Reinforcement Learning (RL), advocating for using world models as simulators to generate experience for training long-term, reusable agent policies.

Building on these critiques, the paper introduces a new architectural framework called PAN (Physical, Agentic, and Nested). PAN is a hierarchical, generative model that uses a mixed representation, features an enhanced LLM backbone for high-level reasoning and a diffusion model for low-level dynamics, and is trained with a generative objective grounded in observation data. The paper positions PAN as a more robust and principled path toward general-purpose world models that can serve as a foundation for truly intelligent agents.

**Rating: 9/10**

For a sophisticated, PhD-level audience, this paper is excellent. It is a well-structured, cogent, and thought-provoking position paper that engages directly and critically with a dominant paradigm in a cutting-edge research area. The arguments are not merely asserted but are supported by clear reasoning and, impressively, by theoretical formalism (e.g., propositions on loss function collapse and a theorem on representation completeness). While it is a critique and proposal, lacking the empirical results of the proposed PAN model (which are promised in a forthcoming paper), its conceptual contribution is significant. It clarifies fundamental debates and provides a clear, constructive, and ambitious alternative, making it a valuable and stimulating read.

### 2. Main Ideas Discussed

1.  **The Purpose of a World Model is Simulating Actionable Possibilities for Reasoning:** The paper's central thesis is that the primary function of a world model is not to be a video generator, but to serve as an internal "sandbox for reasoning and thought-experiment." It should simulate the potential outcomes of an agent's actions to enable purposeful planning and decision-making, reframing the evaluation of such models from visual fidelity to their utility in agentic tasks.
2.  **Critique of Latent-Only Prediction and Defense of Generative, Token-Based Models:** A core contribution is the detailed critique of the JEPA-style philosophy, which favors non-generative models that predict future representations purely in a latent space. The authors argue this approach is flawed because it ignores information-dense data like text, creates brittle representations, and uses objective functions prone to collapse. They advocate for a return to generative principles, where models are grounded by reconstructing observable data, and for using mixed discrete/continuous representations that leverage the power of token-based LLMs for abstract reasoning.
3.  **The PAN (Physical, Agentic, Nested) Architecture as a New Framework:** The paper proposes PAN, a novel architecture that synthesizes its critiques into a constructive solution. PAN is a hierarchical and generative world model that uses a mixed representation (both continuous embeddings and discrete tokens), integrates an LLM backbone for high-level reasoning with diffusion models for low-level dynamics, is trained with a data-reconstruction objective, and is designed to train agents via reinforcement learning. It serves as a concrete blueprint for the paper's vision of a more general and capable world model.

### 3. 10 Most Important Citations

1.  LeCun, Y. 2022. A path towards autonomous machine intelligence version 0.9. 2. This paper outlines the vision for autonomous machine intelligence that the authors critique, particularly its advocacy for non-generative, latent-space predictive architectures (JEPA).
2.  Ha, D. et al. 2018. World models. This is a foundational paper that demonstrated the concept of training an agent entirely within a learned, compressed simulation of its environment, a core idea that this paper builds upon and refines.
3.  Assran, M. et al. 2025. V-jepa 2: Self-supervised video models enable understanding, prediction and planning. This work is cited as a recent, concrete example of the JEPA-style architecture that the authors analyze and critique for its limitations in scope, representation, and objective function.
4.  Brooks, T. et al. 2024. Video generation models as world simulators. This paper on OpenAI's Sora is used as an example of a system that excels at video generation but falls short of being a true world model by the authors' definition, as it lacks interactivity and explicit state representation for planning. [https://openai.com/research/video-generation-models-as-world-simulators](https://openai.com/research/video-generation-models-as-world-simulators)
5.  Silver, D. et al. 2016. Mastering the game of go with deep neural networks and tree search. AlphaGo is presented as a prime example of successful simulative reasoning where a model (MCTS) leverages a known world model (the rules of Go) to perform powerful planning.
6.  Sutton, R. S. et al. 1998. Reinforcement learning: An introduction. This classic textbook is cited for the foundational concepts of reinforcement learning, which the authors advocate as the proper framework for using a world model to train an agent, contrasting it with the limitations of model-predictive control (MPC).
7.  Van Den Oord, A. et al. 2017. Neural discrete representation learning. This paper introduced the VQ-VAE, a key technique for learning the discrete representations that the authors argue are a critical asset for building stable, compositional, and abstract world models.
8.  Ball, L. J. 2020. Hypothetical Thinking. This citation from psychology provides the cognitive science basis for the paper's core argument that intelligent reasoning is fundamentally based on the ability to mentally simulate future possibilities.
9.  Hao, S. et al. 2023. Reasoning with language model is planning with world model. This work is referenced to support the authors' claim that Large Language Models can inherently function as world models, performing planning and reasoning over the discrete, symbolic space of language.
10. Hu, Z. et al. 2025. Pan: A physical, agentic, and nested world model for general and super intelligence. This is the authors' own forthcoming paper, cited as the destination for the full details and results of their proposed PAN architecture, marking it as the culmination of the paper's critiques.
