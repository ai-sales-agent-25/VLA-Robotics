https://arxiv.org/abs/2503.20840

CodeTool: Enhancing Programmatic Tool Invocation of LLMs via Process Supervision

### 1. Summary and Rating

This paper introduces **CodeTool**, a novel framework designed to improve how Large Language Models (LLMs) use external tools. The core problem it addresses is that existing methods for tool invocation, which typically rely on generating JSON or text commands, are often inefficient for complex, multi-step tasks and struggle with verifying the correctness of intermediate steps. Programmatic (code-based) tool invocation is a more powerful alternative but has lacked effective intermediate supervision.

CodeTool's main innovation is a **stepwise code generation** process guided by a **dual-reward system** for process supervision. At each step of solving a task, the LLM generates candidate Python code snippets to call a tool. The framework then selects the best snippet to execute based on two rewards:
1.  **On-the-spot Reward:** An immediate, binary reward (0 or 1) that verifies if the generated code is executable and syntactically correct. This provides a crucial, objective check on the local validity of each step.
2.  **Latent Reward:** A forward-looking reward that estimates the potential of the current step to lead to a successful final solution. This reward is generated by a **Process Reward Model (PRM)**, which is trained on automatically collected process data to predict the long-term utility of an action, thus guiding the LLM towards more efficient reasoning paths.

The authors demonstrate CodeTool's effectiveness through extensive experiments on the StableToolBench and RestBench-TMDB benchmarks. The results show that CodeTool significantly outperforms established baselines like Chain-of-Thought (CoT) and Depth-First Search Decision Tree (DFSDT) in terms of success rate. It is particularly effective when paired with LLMs that have strong native coding abilities, and it reduces the number of required interactions (reasoning depth) compared to JSON-based methods.

**Rating: 9/10**

This is a high-quality, well-executed paper that makes a strong contribution to the field of LLM agents and tool use. The dual-reward mechanism is an elegant and effective solution to the problem of supervising complex, code-based reasoning chains. It cleverly combines an easily verifiable, immediate reward with a learned, predictive reward to balance local correctness and global task progress. The methodology is sound, the experiments are thorough and include strong baselines and insightful ablation studies, and the results convincingly validate the proposed framework. The automated pipeline for training the Process Reward Model is a practical approach that circumvents the need for expensive human annotation. While building on existing ideas of process supervision, its specific application to stepwise code generation for tool use and the dual-reward formulation represents a significant and valuable advancement.

### 2. Main Ideas

1.  **Stepwise Code Generation for Tool Use:** The central idea is to shift from generating entire scripts or simple JSON/Text commands to an iterative, step-by-step process of code generation. This allows the LLM to receive intermediate feedback from tool execution, parse partial results, and dynamically adjust its plan, making the process more robust and suitable for complex tasks that require sequential tool calls.

2.  **Dual-Reward Process Supervision:** The key mechanism is a dual-reward system that guides the model's choices at each step. This system decomplexifies the supervision signal by combining:
    *   An **On-the-spot Reward** based on code executability, which ensures the reasoning path is valid at every step.
    *   A **Latent Reward** from a trained Process Reward Model (PRM), which guides the model along paths that are most likely to lead to a correct final answer, preventing it from getting stuck in valid but unproductive loops.

3.  **Automated Training of a Process Reward Model (PRM):** To enable the Latent Reward without manual annotation, the paper proposes a fully automated method for collecting process data and training the PRM. It uses a search algorithm (a simplified MCTS) to explore different code generation paths, automatically evaluates their final success, and uses this information to train the PRM to predict the utility of intermediate steps, making the entire framework scalable and practical.

### 3. 10 Most Important Citations

1.  **Qin et al. 2023. Toolllm: Facilitating large language models to master 16000+ real-world apis.**
    This citation introduces the ToolBench benchmark and the ToolLLaMA model, which created a large-scale evaluation standard for LLM tool use that this paper heavily relies on.

2.  **Guo et al. 2024. Stabletoolbench: Towards stable large-scale benchmarking on tool learning of large language models.**
    This work provides the primary, more stable benchmark used for the paper's main experiments, addressing reproducibility issues in the original ToolBench.

3.  **Lightman et al. 2023. Let's verify step by step.**
    This is a foundational paper on process supervision, demonstrating that rewarding correct intermediate reasoning steps is highly effective for complex problem-solving, a core principle adopted by CodeTool.

4.  **Wang et al. 2024c. Executable code actions elicit better llm agents.**
    This paper validates the premise that using executable code as the action space for LLM agents is more powerful and flexible than text or JSON, which is the foundational assumption of the CodeTool framework.

5.  **Uesato et al. 2022. Solving math word problems with process- and outcome-based feedback.**
    This is another key reference for process-based supervision, showing its superiority over only rewarding the final outcome and directly influencing the design philosophy of rewarding intermediate steps in CodeTool.

6.  **Yu et al. 2024. Steptool: A step-grained reinforcement learning framework for tool learning in llms.**
    This paper presents a closely related and contemporary baseline that also uses step-level rewards for tool learning, making it a critical point of comparison for establishing CodeTool's novelty and performance.

7.  **Wei et al. 2022. Chain-of-thought prompting elicits reasoning in large language models.**
    This citation introduces Chain-of-Thought (CoT), a fundamental and widely used baseline for multi-step reasoning in LLMs against which CodeTool's performance is measured.

8.  **Yao et al. 2023. React: Synergizing reasoning and acting in language models.**
    This work introduced the influential ReAct framework that interleaves thought and action, providing a conceptual foundation for many modern tool-use paradigms, including the iterative approach in CodeTool.

9.  **Song et al. 2023. Restgpt: Connecting large language models with real-world restful apis.**
    This paper introduces the RestBench-TMDB benchmark, which the authors use to demonstrate the generalization capabilities of the CodeTool framework on a different set of tools and tasks.

10. **Shi et al. 2024. Chain of tools: Large language model is an automatic multi-tool learner.**
    This paper proposes an alternative method for tool use where the LLM generates a complete tool-using plan at once, serving as a methodological contrast to CodeTool's stepwise, supervised generation approach.
