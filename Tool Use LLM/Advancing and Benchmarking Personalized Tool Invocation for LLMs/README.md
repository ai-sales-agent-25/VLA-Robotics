https://arxiv.org/abs/2505.04072

**Advancing and Benchmarking Personalized Tool Invocation for LLMs**

1.  **Brief Summary and Rating:**
    This paper introduces the concept of "Personalized Tool Invocation" for Large Language Models (LLMs), addressing a gap in existing research that primarily focuses on general tool use without considering user-specific needs. The authors define two key tasks: (1) **Tool Preference**, where an LLM must select among functionally similar tools based on user preferences or query context, and (2) **Profile-dependent Query**, where an LLM infers missing tool parameters from a user's profile. To tackle these, they propose **PTool**, an automated data synthesis framework that generates tools, diverse user profiles (including behavioral history), and corresponding query-solution pairs. Using PTool, they construct **PTBench**, the first benchmark for evaluating personalized tool invocation. Experiments demonstrate that fine-tuning open-source LLMs (specifically Qwen2.5-7B-Instruct) on the synthesized data significantly improves their ability to handle these personalized tasks, particularly in tool preference and inferring profile-dependent parameters, often outperforming even larger, proprietary API-based models in these specific aspects. The work provides a novel paradigm, a robust data generation methodology, and a much-needed benchmark for an increasingly important area of LLM research.

    **Rating: 9/10.**
    This paper earns a high rating for its timely and significant contribution. It clearly articulates a crucial, underexplored problem in LLM tool invocationâ€”personalization. The conceptualization of Tool Preference and Profile-dependent Query is sound and practical. The PTool data synthesis framework is well-designed, particularly its hierarchical approach to user profile generation and multi-agent query simulation. The creation of PTBench is a valuable community resource. The experimental validation is thorough, including comparisons with strong baselines, ablation studies, and error analysis, convincingly demonstrating the efficacy of their approach. The work opens up new avenues for research into making LLMs more practically useful and user-centric. The only minor detraction might be the inherent limitations of synthetic data, though the authors mitigate this with manual verification for the test set and demonstrate good generalization.

2.  **Main Ideas Discussed:**
    1.  **Introduction of Personalized Tool Invocation as a Novel Task:** The core idea is that LLMs should not just use tools generically but must adapt their tool selection and parameterization based on individual user profiles, preferences, and historical interactions. This is broken down into two specific challenges:
        *   **Tool Preference:** Enabling LLMs to choose the most appropriate tool when multiple tools offer similar functionalities, by considering user attributes (e.g., price sensitivity, brand loyalty inferred from profile) and current query context.
        *   **Profile-dependent Query:** Equipping LLMs to infer missing but necessary tool parameters directly from a user's profile (e.g., default delivery address, preferred sorting order) when a user's query is underspecified.
    2.  **PTool Data Synthesis Framework and PTBench Benchmark:** To enable research and evaluation in personalized tool invocation, the paper proposes:
        *   **PTool:** An automated framework for generating diverse, high-quality data. It involves three stages: (i) Tool Generation (creating a library of tools with varied functionalities and platform affiliations), (ii) User Profile Construction (systematically generating user profiles with basic features, implicit preferences, and simulated behavioral histories), and (iii) Query and Solution Generation (using a multi-agent LLM setup to simulate user queries and corresponding personalized tool invocation solutions, followed by verification).
        *   **PTBench:** The first benchmark derived from PTool, specifically designed for evaluating LLMs on personalized tool invocation tasks, with manually verified samples in the test set.
    3.  **Effectiveness of Fine-tuning for Personalized Tool Invocation:** The paper demonstrates through extensive experiments that fine-tuning existing open-source LLMs (like Qwen2.5-7B-Instruct) on data generated by PTool significantly enhances their capabilities in personalized tool invocation. These fine-tuned models show marked improvements in selecting user-preferred tools and inferring parameters from user profiles, outperforming pre-trained models and even large API-based models in these specific personalization aspects, without compromising their general abilities.

3.  **10 Most Important Citations:**
    1.  Schick et al. 2024. Toolformer: Language models can teach themselves to use tools. This paper is foundational for LLM tool use, demonstrating that models can learn to use external tools through API calls.
    2.  Qin et al. 2024. ToolLLM: Facilitating large language models to master 16000+ real-world APIs. This work is highly relevant for its focus on enabling LLMs to use a vast number of real-world APIs and likely involves sophisticated data collection/synthesis for tool learning.
    3.  Liu et al. 2025. ToolACE: Enhancing function calling with accuracy, complexity, and diversity. This citation is important as the authors state their PTool framework's API tree generation is similar to ToolACE, indicating an influence on their data synthesis methodology for tool generation.
    4.  Zhang et al. 2024c. Personalization of large language models: A survey. This survey directly addresses the broader context of personalizing LLMs, making it a key reference for the paper's core theme.
    5.  Zhao et al. 2023. A survey of large language models. This provides general background on LLM capabilities and is cited in the introduction to establish the context of LLM advancements.
    6.  Yao et al. 2022. React: Synergizing reasoning and acting in language models. This paper introduced an important paradigm for LLMs to combine reasoning and action, which is fundamental to effective tool invocation.
    7.  Hu et al. 2022. LoRA: Low-rank adaptation of large language models. This citation is crucial as LoRA is the parameter-efficient fine-tuning technique employed by the authors to train their models, making their experimental setup feasible.
    8.  Team, Qwen. 2024a. Qwen2 technical report. This is the technical report for the base model (Qwen2.5-7B-Instruct) that the authors fine-tuned, making it central to their experimental results.
    9.  Hao et al. 2024. Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings. This represents another significant approach to tool invocation, providing context for the state-of-the-art in the field.
    10. Nakano et al. 2021. Webgpt: Browser-assisted question-answering with human feedback. This is an early and influential work on LLMs using external tools (a web browser) to access information, setting a precedent for tool-augmented LLMs.
